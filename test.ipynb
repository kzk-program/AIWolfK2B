{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.val = 1\n",
    "test = Test()\n",
    "\n",
    "try:\n",
    "    if test.nonval == 1:\n",
    "        print(\"Exists\")\n",
    "except AttributeError:\n",
    "    0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5を使ったJP to Protocolのテストコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pathlib pandas torch transformers sentencepiece scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なパッケージのインポート\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sonoisa/t5-base-japanese\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_length_src = 400\n",
    "max_length_target = 200\n",
    "\n",
    "batch_size_train = 8\n",
    "batch_size_valid = 8\n",
    "\n",
    "epochs = 1000\n",
    "patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/content/drive/MyDrive/google_colaboratory/document_summarization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_data = pd.read_csv(data_dir_path.joinpath('body_data.csv'))\n",
    "summary_data = pd.read_csv(data_dir_path.joinpath('summary_data.csv'))\n",
    "\n",
    "pd.merge(\n",
    "    body_data.query('text.notnull()', engine='python').rename(columns={'text': 'body'}),\n",
    "    summary_data.rename(columns={'text': 'summary'}),\n",
    "    on='article_id', how='inner'\n",
    ").sort_values('article_id').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_text(x, add_char='。'):\n",
    "    return add_char.join(x)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "    text = neologdn.normalize(text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "summary_data = summary_data.query('text.notnull()', engine='python').groupby(\n",
    "    'article_id'\n",
    ").agg({'text': join_text})\n",
    "\n",
    "body_data = body_data.query('text.notnull()', engine='python')\n",
    "\n",
    "data = pd.merge(\n",
    "    body_data.rename(columns={'text': 'body_text'}),\n",
    "    summary_data.rename(columns={'text': 'summary_text'}),\n",
    "    on='article_id', how='inner'\n",
    ").assign(\n",
    "    body_text=lambda x: x.body_text.map(lambda y: preprocess_text(y)),\n",
    "    summary_text=lambda x: x.summary_text.map(lambda y: preprocess_text(y))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batch_data(train_data, valid_data, tokenizer):\n",
    "\n",
    "    def generate_batch(data):\n",
    "\n",
    "        batch_src, batch_tgt = [], []\n",
    "        for src, tgt in data:\n",
    "            batch_src.append(src)\n",
    "            batch_tgt.append(tgt)\n",
    "\n",
    "        batch_src = tokenizer(\n",
    "            batch_src, max_length=settings.max_length_src, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        batch_tgt = tokenizer(\n",
    "            batch_tgt, max_length=settings.max_length_target, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return batch_src, batch_tgt\n",
    "\n",
    "    train_iter = DataLoader(train_data, batch_size=settings.batch_size_train, shuffle=True, collate_fn=generate_batch)\n",
    "    valid_iter = DataLoader(valid_data, batch_size=settings.batch_size_valid, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "    return train_iter, valid_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(settings.MODEL_NAME, is_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['body_text'], data['summary_text'], test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "train_data = [(src, tgt) for src, tgt in zip(X_train, y_train)]\n",
    "valid_data = [(src, tgt) for src, tgt in zip(X_test, y_test)]\n",
    "\n",
    "train_iter, valid_iter = convert_batch_data(train_data, valid_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class T5FineTuner(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(settings.MODEL_NAME)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
    "        decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, PAD_IDX):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loop = 1\n",
    "    losses = 0\n",
    "    pbar = tqdm(data)\n",
    "    for src, tgt in pbar:\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        labels = tgt['input_ids'].to(settings.device)\n",
    "        labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=src['input_ids'].to(settings.device),\n",
    "            attention_mask=src['attention_mask'].to(settings.device),\n",
    "            decoder_attention_mask=tgt['attention_mask'].to(settings.device),\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs['loss']\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "        pbar.set_postfix(loss=losses / loop)\n",
    "        loop += 1\n",
    "        \n",
    "    return losses / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, PAD_IDX):\n",
    "    \n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data:\n",
    "\n",
    "            labels = tgt['input_ids'].to(settings.device)\n",
    "            labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=src['input_ids'].to(settings.device),\n",
    "                attention_mask=src['attention_mask'].to(settings.device),\n",
    "                decoder_attention_mask=tgt['attention_mask'].to(settings.device),\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs['loss']\n",
    "            losses += loss.item()\n",
    "        \n",
    "    return losses / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5FineTuner()\n",
    "model = model.to(settings.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "PAD_IDX = tokenizer.pad_token_id\n",
    "best_loss = float('Inf')\n",
    "best_model = None\n",
    "counter = 1\n",
    "\n",
    "for loop in range(1, settings.epochs + 1):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    loss_train = train(model=model, data=train_iter, optimizer=optimizer, PAD_IDX=PAD_IDX)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    loss_valid = evaluate(model=model, data=valid_iter, PAD_IDX=PAD_IDX)\n",
    "\n",
    "    print('[{}/{}] train loss: {:.4f}, valid loss: {:.4f} [{}{:.0f}s] counter: {} {}'.format(\n",
    "        loop, settings.epochs, loss_train, loss_valid,\n",
    "        str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "        elapsed_time % 60,\n",
    "        counter,\n",
    "        '**' if best_loss > loss_valid else ''\n",
    "    ))\n",
    "\n",
    "    if best_loss > loss_valid:\n",
    "        best_loss = loss_valid\n",
    "        best_model = copy.deepcopy(model)\n",
    "        counter = 1\n",
    "    else:\n",
    "        if counter > settings.patience:\n",
    "            break\n",
    "\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_path = Path('model')\n",
    "if not model_dir_path.exists():\n",
    "    model_dir_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(model_dir_path)\n",
    "best_model.model.save_pretrained(model_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_model(text, trained_model, tokenizer, num_return_sequences=1):\n",
    "\n",
    "    trained_model.eval()\n",
    "    \n",
    "    text = preprocess_text(text)\n",
    "    batch = tokenizer(\n",
    "        [text], max_length=settings.max_length_src, truncation=True, padding=\"longest\", return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 生成処理を行う\n",
    "    outputs = trained_model.generate(\n",
    "        input_ids=batch['input_ids'].to(settings.device),\n",
    "        attention_mask=batch['attention_mask'].to(settings.device),\n",
    "        max_length=settings.max_length_target,\n",
    "        repetition_penalty=8.0,   # 同じ文の繰り返し（モード崩壊）へのペナルティ\n",
    "        # temperature=1.0,  # 生成にランダム性を入れる温度パラメータ\n",
    "        # num_beams=10,  # ビームサーチの探索幅\n",
    "        # diversity_penalty=1.0,  # 生成結果の多様性を生み出すためのペナルティパラメータ\n",
    "        # num_beam_groups=10,  # ビームサーチのグループ\n",
    "        num_return_sequences=num_return_sequences,  # 生成する文の数\n",
    "    )\n",
    "\n",
    "    generated_texts = [\n",
    "        tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in outputs\n",
    "    ]\n",
    "\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_dir_path)\n",
    "trained_model = T5ForConditionalGeneration.from_pretrained(model_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trained_model.to(settings.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "body = valid_data[index][0]\n",
    "summaries = valid_data[index][1]\n",
    "generated_texts = generate_text_from_model(\n",
    "    text=body, trained_model=trained_model, tokenizer=tokenizer, num_return_sequences=1\n",
    ")\n",
    "print('□ 生成本文')\n",
    "print('\\n'.join(generated_texts[0].split('。')))\n",
    "print()\n",
    "print('□ 教師データ要約')   \n",
    "print('\\n'.join(summaries.split('。')))\n",
    "print()\n",
    "print('□ 本文')\n",
    "print(body)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enumをlistやdictから作る\n",
    "参考：https://stackoverflow.com/questions/62120732/generate-an-enum-class-from-a-list-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "def generate_enum(enumClass, enumDict):\n",
    "    \"\"\"\n",
    "    Generates python code for an Enum\n",
    "    \"\"\"\n",
    "\n",
    "    enum_template = \"\"\"\n",
    "@unique\n",
    "class {enumClass}(Enum)\n",
    "{enumBody}\n",
    "\"\"\"\n",
    "\n",
    "    enumBody = '\\n'.join([f\"    {name} = '{value}'\" for (name,value) in enumDict.items()])\n",
    "\n",
    "    return enum_template.format(enumClass=enumClass,enumBody=enumBody)\n",
    "species_list = ['HUMAN',\"WEREWOLF\",\"ANY\"]\n",
    "print(generate_enum(\"ProtocolToken\",species_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from enum import Enum\n",
    "species_list = ['HUMAN',\"WEREWOLF\",\"ANY\"]\n",
    "ProtocolToken = Enum('ProtocolToken',species_list)\n",
    "list(ProtocolToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "subject_list = [\"Agent01\",\"Agent02\", \"Agent03\", \"Agent04\", \"Agent05\",\n",
    "                   \"Agent06\", \"Agent07\", \"Agent08\", \"Agent09\", \"Agent10\", \n",
    "                   \"Agent11\", \"Agent12\", \"Agent13\", \"Agent14\", \"Agent15\",\"UNSPEC\",\"ANY\"] #TODO:ここ周りテキトーにやってる。ホントは分類ではなく値自身を使えばいいはず\n",
    "verb_list = ['ESTIMATE', 'COMINGOUT', 'DIVINATION', 'GUARD', 'VOTE',\n",
    "            'ATTACK', 'DIVINED', 'IDENTIFIED', 'GUARDED', 'VOTED',\n",
    "            'ATTACKED', 'AGREE', 'DISAGREE', 'Skip', 'Over' ] # REVIEW: Skip, Overをuppercaseにする必要があるかどうか\n",
    "target_list = subject_list\n",
    "species_list = ['HUMAN',\"WEREWOLF\",\"ANY\"]\n",
    "role_list = ['VILLAGER','SEER', 'MEDIUM','BODYGUARD','WEREWOLF','POSSESSED','ANY']\n",
    "\n",
    "protocol_token_list = subject_list + verb_list + target_list + species_list + role_list\n",
    "protocol_token_dict = {token: i for i, token in enumerate(protocol_token_list)}\n",
    "\n",
    "ProtocolToken = Enum('ProtocolToken',protocol_token_dict)\n",
    "list(ProtocolToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_terminal_tokens(partial_sentence: str):\n",
    "    terminal_tokens = {\n",
    "        'sentence_start': ['Skip', 'Over', 'Agent', 'ANY', 'UNSPEC'],\n",
    "        'VTR_VT_VTS_AGG_OTS_OS1_OS2_OSS_DAY': ['ESTIMATE', 'COMMINGOUT', 'DIVINATION', 'GUARD', 'VOTE', 'ATTACK', 'GUARDED', 'VOTED', 'ATTACKED', 'DIVINED', 'IDENTIFIED', 'Agree', 'Disagree', 'REQUEST', 'INQUIRE', 'NOT', 'BECAUSE', 'XOR', 'AND', 'OR', 'DAY'],\n",
    "        'TR': ['Agent', 'ANY'],\n",
    "        'T': ['Agent', 'ANY'],\n",
    "        'TSp': ['Agent', 'ANY'],\n",
    "        'TSe': ['Agent', 'ANY'],\n",
    "        'S2': ['Skip', 'Over', 'Agent', 'ANY', 'UNSPEC'],\n",
    "        'SS': ['Skip', 'Over', 'Agent', 'ANY', 'UNSPEC'],\n",
    "        'recsentence': ['Skip', 'Over', 'Agent', 'ANY', 'UNSPEC'],\n",
    "        'rec2sentence': ['Skip', 'Over', 'Agent', 'ANY', 'UNSPEC', 'eps'],\n",
    "        'species': ['HUMAN', 'WEREWOLF', 'ANY'],\n",
    "        'role': ['VILLAGER', 'SEER', 'MEDIUM', 'BODYGUARD', 'WEREWOLF', 'POSSESSED'],\n",
    "        'talk_number': ['day'],\n",
    "        'agent_number': [str(i) for i in range(1, 16)],\n",
    "        'day_number': [str(i) for i in range(1, 1000)], # ここでは上限を1000に設定していますが、適宜変更してください\n",
    "        'ID_number': [str(i) for i in range(1, 1000)] # ここでは上限を1000に設定していますが、適宜変更してください\n",
    "    }\n",
    "\n",
    "    # 入力された部分文から次の非終端記号を特定する\n",
    "    for non_terminal, tokens in terminal_tokens.items():\n",
    "        for token in tokens:\n",
    "            if partial_sentence.endswith(token):\n",
    "                next_non_terminal = non_terminal\n",
    "                break\n",
    "\n",
    "    # 次の非終端記号に対応する終端記号の一覧を返す\n",
    "    return terminal_tokens[next_non_terminal]\n",
    "\n",
    "# 使用例\n",
    "partial_sentence = \"Agent1 ESTIMATE Agent2\"\n",
    "print(next_terminal_tokens(partial_sentence)) # ['VILLAGER', 'SEER', 'MEDIUM', 'BODYGUARD', 'WEREWOLF', 'POSSESSED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類するラベルのリスト\n",
    "subject_list = [\"Agent\",\"UNSPEC\",\"ANY\"] #TODO:ここ周りテキトーにやってる。ホントは分類ではなく値自身を使えばいいはず\n",
    "verb_list = ['ESTIMATE', 'COMINGOUT', 'DIVINATION', 'GUARD', 'VOTE',\n",
    "            'ATTACK', 'DIVINED', 'IDENTIFIED', 'GUARDED', 'VOTED',\n",
    "            'ATTACKED', 'AGREE', 'DISAGREE', 'Skip', 'Over' ] # REVIEW: Skip, Overをuppercaseにする必要があるかどうか\n",
    "target_list = subject_list\n",
    "species_list = ['HUMAN',\"WEREWOLF\",\"ANY\"]\n",
    "role_list = ['VILLAGER','SEER', 'MEDIUM','BODYGUARD','WEREWOLF','POSSESSED','ANY']\n",
    "\n",
    "protocol_token_list = subject_list + verb_list + target_list + species_list + role_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのインポート\n",
    "# 事前学習済みモデル\n",
    "PRETRAINED_MODEL_NAME = \"sonoisa/t5-base-english-japanese\" #\"sonoisa/t5-base-japanese\"\n",
    "\n",
    "# 転移学習済みモデル\n",
    "MODEL_DIR = \"/content/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, LogitsProcessorList, LogitsProcessor\n",
    "import numpy as np\n",
    "\n",
    "model :T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "tokenizer :T5Tokenizer = T5Tokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, LogitsProcessorList, LogitsProcessor\n",
    "import numpy as np\n",
    "\n",
    "model :T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "tokenizer :T5Tokenizer = T5Tokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#書くprotocolがtokenizerのvocabに含まれているかどうかを確認する\n",
    "for p in protocol_token_list:\n",
    "    if p not in tokenizer.get_vocab().keys():\n",
    "        print(f\"protocol:{p}\", f\"{tokenizer.encode(p)},{tokenizer.decode(tokenizer.encode(p))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#protocolをlower caseに <-したほうが良さそう\n",
    "lower_protocol_token_list = [p.lower() for p in protocol_token_list]\n",
    "#書くprotocolがtokenizerのvocabに含まれているかどうかを確認する\n",
    "for p in lower_protocol_token_list:\n",
    "    if p not in tokenizer.get_vocab().keys():\n",
    "        print(f\"some tokens protocol:{p}\", f\"{tokenizer.tokenize(p)},{tokenizer.encode(p)},{tokenizer.decode(tokenizer.encode(p))}\")\n",
    "    else:\n",
    "        print(f\"one token protocol:{p}\", f\"{tokenizer.tokenize(p)},{tokenizer.encode(p)},{tokenizer.decode(tokenizer.encode(p))}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()['agree'] # なぜかagreeはvocabにない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_scores_to_inf_for_banned_tokens(scores, banned_tokens):\n",
    "    \"\"\"\n",
    "    Modifies the scores in place by setting the banned token positions to `-inf`. Banned token is expected to be a\n",
    "    list of list of banned tokens to ban in the format [[batch index, vocabulary position],...\n",
    "\n",
    "    Args:\n",
    "        scores: logits distribution of shape (batch size, vocabulary size)\n",
    "        banned_tokens: list of list of tokens to ban of length (batch_size)\n",
    "    \"\"\"\n",
    "    banned_mask_list = []\n",
    "    for idx, batch_banned_tokens in enumerate(banned_tokens):\n",
    "        for token in batch_banned_tokens:\n",
    "            banned_mask_list.append([idx, token])\n",
    "    if not banned_mask_list:\n",
    "        return scores\n",
    "\n",
    "    banned_mask = torch.LongTensor(banned_mask_list)\n",
    "    indices = torch.ones(len(banned_mask))\n",
    "\n",
    "    banned_mask = (\n",
    "        torch.sparse.LongTensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()\n",
    "    )\n",
    "    scores = scores.masked_fill(banned_mask, -float(\"inf\"))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvenLogits(LogitsProcessor):\n",
    "  def __call__(self, input_ids, scores):\n",
    "\n",
    "    banned_tokens = []\n",
    "    for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):\n",
    "      elementwise_length = np.vectorize(len)\n",
    "      keys = np.array(list(tokenizer.vocab.keys()))\n",
    "      values = np.array(list(tokenizer.vocab.values()))\n",
    "\n",
    "      # indexes of tokens that are too long\n",
    "      indexes = np.where(elementwise_length(keys) % 2 == 0)[0]\n",
    "\n",
    "      banned_tokens.append(values[indexes])\n",
    "\n",
    "    scores = set_scores_to_inf_for_banned_tokens(scores, banned_tokens)\n",
    "    return scores\n",
    "\n",
    "class ABCLogits(LogitsProcessor):\n",
    "  def __init__(self, vocab):\n",
    "    \"\"\"\n",
    "    vocab is a dictionary where the keys are tokens\n",
    "    and the values are the corresponding ids.\n",
    "    \"\"\"\n",
    "    # create an array of tokens\n",
    "    # remove the 'Ġ' token (used to represent a blank space in the tokenizer)\n",
    "    self.keys = list(tokenizer.get_vocab().keys())\n",
    "    # index_to_pop = self.keys.index(' ') \n",
    "    # self.keys.pop(index_to_pop)\n",
    "    self.keys = np.array(self.keys)\n",
    "\n",
    "    # create an array of ids\n",
    "    # also remove the 'Ġ' token\n",
    "    self.values = list(tokenizer.get_vocab().values())\n",
    "    # self.values.pop(index_to_pop)\n",
    "    self.values = np.array(self.values)\n",
    "\n",
    "    # vectorized function used to get the first character of a token\n",
    "    # ignores leading whitespaces and 'Ġ' tokens\n",
    "    first_char = lambda x: x.strip('Ġ ')[0].lower()\n",
    "    self.first_char = np.vectorize(first_char)\n",
    "\n",
    "    # get the indexes of all IDs that do not start with the given letter\n",
    "    not_a_indexes = np.where(self.first_char(self.keys) != 'a')\n",
    "    not_b_indexes = np.where(self.first_char(self.keys) != 'b')\n",
    "    not_c_indexes = np.where(self.first_char(self.keys) != 'c')\n",
    "\n",
    "    # create sets of tokens that do not start with 'a', 'b' or 'c'\n",
    "    self.not_a_values = self.values[not_a_indexes]\n",
    "    self.not_b_values = self.values[not_b_indexes]\n",
    "    self.not_c_values = self.values[not_c_indexes]\n",
    "\n",
    "  def __call__(self, input_ids, scores):\n",
    "    banned_tokens = []\n",
    "    # for every beam (partially generated sentence)\n",
    "    for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):\n",
    "      # get the last token of this beam\n",
    "      last_word = tokenizer.decode(beam_input_ids[-1])\n",
    "      # get the first character of this last token\n",
    "      starting_char = self.first_char(last_word)\n",
    "      # if the last token starts with 'a',\n",
    "      # ban all words that do not start with 'b', etc.\n",
    "      if starting_char == 'a':\n",
    "        banned_tokens.append(self.not_b_values)\n",
    "      elif starting_char == 'b':\n",
    "        banned_tokens.append(self.not_c_values)\n",
    "      elif starting_char == 'c':\n",
    "        banned_tokens.append(self.not_a_values)\n",
    "      else:\n",
    "        banned_tokens.append(self.not_a_values)\n",
    "    # set the scores of all banned tokens over the beams to -inf\n",
    "    scores = set_scores_to_inf_for_banned_tokens(scores, banned_tokens)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LL1Grammar2:\n",
    "    def __init__(self):\n",
    "        self.state = 'S'\n",
    "\n",
    "    def next_valid_tokens(self, token):\n",
    "        if self.state == 'S':\n",
    "            if token == 'hello':\n",
    "                self.state = 'A'\n",
    "                return ['sec', 'apple']\n",
    "            else:\n",
    "                return []\n",
    "        elif self.state == 'A':\n",
    "            if token == 'sec':\n",
    "                self.state = 'B'\n",
    "                return ['cost', 'done']\n",
    "            elif token == 'apple':\n",
    "                self.state = 'A'\n",
    "                return ['sec', 'apple']\n",
    "            else:\n",
    "                return []\n",
    "        elif self.state == 'B':\n",
    "            if token == 'cost':\n",
    "                self.state = 'B'\n",
    "                return ['cost', 'done']\n",
    "            elif token == 'done':\n",
    "                self.state = 'A'\n",
    "                return ['sec', 'apple', 'bad']\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrainedLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, grammar, tokenizer):\n",
    "        super().__init__()\n",
    "        self.grammar = grammar\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        print(input_ids)\n",
    "        last_token_id = input_ids[-1]\n",
    "        last_token = self.tokenizer.convert_ids_to_tokens(last_token_id)\n",
    "        #print(last_token)\n",
    "        valid_tokens = self.grammar.next_valid_tokens(last_token)\n",
    "        valid_token_ids = set(self.tokenizer.convert_tokens_to_ids(valid_tokens))\n",
    "\n",
    "        for token_id in range(scores.shape[-1]):\n",
    "            if token_id not in valid_token_ids:\n",
    "                scores[..., token_id] = float('-inf')\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_str = \"Agent1はAgent2を人狼だと思っている\"\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "grammar = LL1Grammar2()\n",
    "force_words_ids = tokenizer(lower_protocol_token_list, add_special_tokens=False).input_ids\n",
    "# Initialize the custom logits processor\n",
    "logits_processor = LogitsProcessorList([\n",
    "    ConstrainedLogitsProcessor(grammar, tokenizer)\n",
    "])\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    force_words_ids=force_words_ids,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    "    logits_processor=logits_processor\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria\n",
    ")\n",
    "# how many beams to track during the Viterbi algorithm\n",
    "num_beams = 10\n",
    "# how many beams to return after the algorithm\n",
    "num_return_beams = 10\n",
    "\n",
    "# the prompt to continue\n",
    "prompt = 'My cute dog is a'\n",
    "\n",
    "# tokenizing the prompt\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "# instantiating a BeamSearchScorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "# instantiating a list of LogitsProcessor instances\n",
    "# using our custom ABCLogits class\n",
    "logits_processor = LogitsProcessorList([ABCLogits(tokenizer.vocab_size)])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    input_ids=torch.cat([prompt_tokenized] * num_beams).to(dtype = torch.long),\n",
    "    beam_scorer =beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=12)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  print(f'beam {index}: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LL1Parser:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "        self.first_sets = self.compute_first_sets()\n",
    "        self.parse_table = self.construct_parse_table()\n",
    "\n",
    "    def compute_first_sets(self):\n",
    "        first_sets = {}\n",
    "        for non_terminal in self.grammar:\n",
    "            first_sets[non_terminal] = set()\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            for non_terminal, rules in self.grammar.items():\n",
    "                for rule in rules:\n",
    "                    rule_symbols = rule.split()\n",
    "                    first_symbol = rule_symbols[0]\n",
    "\n",
    "                    if first_symbol not in self.grammar:  # Terminal symbol\n",
    "                        if first_symbol not in first_sets[non_terminal]:\n",
    "                            first_sets[non_terminal].add(first_symbol)\n",
    "                            changed = True\n",
    "                    else:  # Non-terminal symbol\n",
    "                        for symbol in first_sets[first_symbol]:\n",
    "                            if symbol not in first_sets[non_terminal]:\n",
    "                                first_sets[non_terminal].add(symbol)\n",
    "                                changed = True\n",
    "        return first_sets\n",
    "\n",
    "    def construct_parse_table(self):\n",
    "        parse_table = {}\n",
    "        for non_terminal in self.grammar:\n",
    "            parse_table[non_terminal] = {}\n",
    "            for terminal in self.first_sets[non_terminal]:\n",
    "                for rule in self.grammar[non_terminal]:\n",
    "                    if rule.startswith(terminal):\n",
    "                        parse_table[non_terminal][terminal] = rule\n",
    "        return parse_table\n",
    "\n",
    "    def parse(self, input_string):\n",
    "        tokens = re.findall(r\"\\w+\", input_string) + [\"$\"]\n",
    "        stack = [list(self.grammar.keys())[0], \"$\"]\n",
    "        cursor = 0\n",
    "\n",
    "        while stack:\n",
    "            top = stack.pop()\n",
    "            if top in self.grammar:  # Non-terminal\n",
    "                if cursor < len(tokens) and tokens[cursor] in self.parse_table[top]:\n",
    "                    production = self.parse_table[top][tokens[cursor]]\n",
    "                    for symbol in reversed(production.split()):\n",
    "                        stack.append(symbol)\n",
    "                else:\n",
    "                    return False, self.first_sets[top]\n",
    "            elif top == tokens[cursor]:\n",
    "                cursor += 1\n",
    "                if cursor == len(tokens) and stack[-1] == \"$\":\n",
    "                    return True, set()\n",
    "            else:\n",
    "                return False, self.first_sets.get(top, set())\n",
    "\n",
    "        return False, set()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grammar = {\n",
    "        \"S\": [\"A a\", \"b B\"],\n",
    "        \"A\": [\"a\"],\n",
    "        \"B\": [\"b\"],\n",
    "    }\n",
    "\n",
    "    parser = LL1Parser(grammar)\n",
    "\n",
    "    test_cases = [\n",
    "        (\"aa\", True),\n",
    "        (\"ba\", False),\n",
    "        (\"bb\", True),\n",
    "        (\"ab\", False),\n",
    "        (\"b\", False),\n",
    "    ]\n",
    "\n",
    "    for string, expected in test_cases:\n",
    "        result, next_symbols = parser.parse(string)\n",
    "        print(f\"Input: {string}, Expected: {expected}, Result: {result}, Next symbols: {next_symbols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class LL1Parser:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "        self.first_sets = self.compute_first_sets()\n",
    "        self.parse_table = self.construct_parse_table()\n",
    "\n",
    "    def compute_first_sets(self):\n",
    "        first_sets = {}\n",
    "        for non_terminal in self.grammar:\n",
    "            first_sets[non_terminal] = set()\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            for non_terminal, rules in self.grammar.items():\n",
    "                for rule in rules:\n",
    "                    rule_symbols = rule.split()\n",
    "                    first_symbol = rule_symbols[0]\n",
    "\n",
    "                    if first_symbol not in self.grammar:  # Terminal symbol\n",
    "                        if first_symbol not in first_sets[non_terminal]:\n",
    "                            first_sets[non_terminal].add(first_symbol)\n",
    "                            changed = True\n",
    "                    else:  # Non-terminal symbol\n",
    "                        for symbol in first_sets[first_symbol]:\n",
    "                            if symbol not in first_sets[non_terminal]:\n",
    "                                first_sets[non_terminal].add(symbol)\n",
    "                                changed = True\n",
    "        return first_sets\n",
    "\n",
    "    def construct_parse_table(self):\n",
    "        parse_table = {}\n",
    "        for non_terminal in self.grammar:\n",
    "            parse_table[non_terminal] = {}\n",
    "            for rule in self.grammar[non_terminal]:\n",
    "                first_symbol = rule.split()[0]\n",
    "                if first_symbol not in self.grammar:  # Terminal symbol\n",
    "                    parse_table[non_terminal][first_symbol] = rule\n",
    "                else:\n",
    "                    for symbol in self.first_sets[first_symbol]:\n",
    "                        parse_table[non_terminal][symbol] = rule\n",
    "        return parse_table\n",
    "\n",
    "    def parse(self, input_string):\n",
    "        tokens = re.findall(r\"\\w+\", input_string) + [\"$\"]\n",
    "        stack = [list(self.grammar.keys())[0], \"$\"]\n",
    "        cursor = 0\n",
    "\n",
    "        while stack:\n",
    "            top = stack.pop()\n",
    "            if top in self.grammar:  # Non-terminal\n",
    "                if cursor < len(tokens) and tokens[cursor] in self.parse_table[top]:\n",
    "                    production = self.parse_table[top][tokens[cursor]]\n",
    "                    for symbol in reversed(production.split()):\n",
    "                        stack.append(symbol)\n",
    "                else:\n",
    "                    return False, self.first_sets[top]\n",
    "            elif top == tokens[cursor]:\n",
    "                cursor += 1\n",
    "                if cursor == len(tokens) and not stack:\n",
    "                    return True, set()\n",
    "            else:\n",
    "                return False, set()\n",
    "\n",
    "        return False, set()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grammar = {\n",
    "        \"S\": [\"A a\", \"b B\"],\n",
    "        \"A\": [\"a\"],\n",
    "        \"B\": [\"b\"],\n",
    "    }\n",
    "\n",
    "    parser = LL1Parser(grammar)\n",
    "\n",
    "    test_cases = [\n",
    "        (\"aa\", True),\n",
    "        (\"ba\", False),\n",
    "        (\"bb\", True),\n",
    "        (\"ab\", False),\n",
    "        (\"b\", False),\n",
    "    ]\n",
    "\n",
    "    for string, expected in test_cases:\n",
    "        result, next_symbols = parser.parse(string)\n",
    "        print(f\"Input: {string}, Expected: {expected}, Result: {result}, Next symbols: {next_symbols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class LL1Parser:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "        self.first_sets = self.compute_first_sets()\n",
    "        self.follow_sets = self.compute_follow_sets()\n",
    "\n",
    "    def compute_first_sets(self):\n",
    "        first_sets = defaultdict(set)\n",
    "\n",
    "        for non_terminal in self.grammar:\n",
    "            first_sets[non_terminal] = self.compute_first_set(non_terminal)\n",
    "\n",
    "        return first_sets\n",
    "\n",
    "    def compute_first_set(self, symbol):\n",
    "        if symbol.isupper():\n",
    "            return {symbol}\n",
    "        \n",
    "        if not symbol in self.grammar:\n",
    "            return {symbol}\n",
    "\n",
    "        first_set = set()\n",
    "        for production in self.grammar[symbol]:\n",
    "            first_symbol = production[0]\n",
    "            if first_symbol.isupper():\n",
    "                first_set.add(first_symbol)\n",
    "            else:\n",
    "                first_set |= self.compute_first_set(first_symbol)\n",
    "\n",
    "        return first_set\n",
    "\n",
    "    # 他の関数は変更なし\n",
    "\n",
    "\n",
    "    def compute_follow_sets(self):\n",
    "        follow_sets = defaultdict(set)\n",
    "\n",
    "        for non_terminal in self.grammar:\n",
    "            for production in self.grammar[non_terminal]:\n",
    "                for i, symbol in enumerate(production):\n",
    "                    if symbol.isupper():\n",
    "                        if i + 1 < len(production):\n",
    "                            next_symbol = production[i + 1]\n",
    "                            if next_symbol.isupper():\n",
    "                                follow_sets[symbol].add(next_symbol)\n",
    "                            else:\n",
    "                                follow_sets[symbol] |= self.first_sets[next_symbol]\n",
    "                        else:\n",
    "                            follow_sets[symbol] |= self.compute_follow_set(non_terminal)\n",
    "\n",
    "        return follow_sets\n",
    "\n",
    "    def compute_follow_set(self, non_terminal):\n",
    "        follow_set = set()\n",
    "\n",
    "        for nt in self.grammar:\n",
    "            for production in self.grammar[nt]:\n",
    "                if non_terminal in production:\n",
    "                    idx = production.index(non_terminal)\n",
    "                    if idx + 1 < len(production):\n",
    "                        next_symbol = production[idx + 1]\n",
    "                        if next_symbol.isupper():\n",
    "                            follow_set.add(next_symbol)\n",
    "                        else:\n",
    "                            follow_set |= self.first_sets[next_symbol]\n",
    "                    else:\n",
    "                        if nt != non_terminal:\n",
    "                            follow_set |= self.compute_follow_set(nt)\n",
    "\n",
    "        return follow_set\n",
    "\n",
    "    def parse(self, input_str):\n",
    "        stack = [\"$\"]\n",
    "        stack.extend(list(reversed(list(self.grammar[\"S\"][0]))))\n",
    "        input_str = list(input_str) + [\"$\"]\n",
    "\n",
    "        while input_str:\n",
    "            if stack[-1].isupper():\n",
    "                top_symbol = stack.pop()\n",
    "                input_symbol = input_str[0]\n",
    "\n",
    "                for production in self.grammar[top_symbol]:\n",
    "                    first_set = self.compute_first_set(production[0])\n",
    "\n",
    "                    if input_symbol in first_set:\n",
    "                        stack.extend(list(reversed(production)))\n",
    "                        break\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                if stack[-1] == input_str[0]:\n",
    "                    stack.pop()\n",
    "                    input_str.pop(0)\n",
    "                else:\n",
    "                    return stack[-1:]\n",
    "\n",
    "        return True\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    # BNF表記された文法\n",
    "    grammar = {\n",
    "        \"S\": [[\"E\"]],\n",
    "        \"E\": [[\"T\", \"E'\"]],\n",
    "        \"E'\": [[\"+\", \"T\", \"E'\"], [\"\"]],\n",
    "        \"T\": [[\"F\", \"T'\"]],\n",
    "        \"T'\": [[\"*\", \"F\", \"T'\"], [\"\"]],\n",
    "        \"F\": [[\"(\", \"E\", \")\"], [\"a\"]]\n",
    "    }\n",
    "\n",
    "    parser = LL1Parser(grammar)\n",
    "\n",
    "    # テストケース\n",
    "    test_cases = [\n",
    "        (\"a+a\", True),\n",
    "        (\"a+a*a\", True),\n",
    "        (\"(a+a)*a\", True),\n",
    "        (\"a+a)\", False),\n",
    "        (\"a+)\", [\"+\", \"a\"]),\n",
    "        (\"a+*\", False),\n",
    "        (\"(a+\", [\")\", \"a\"]),\n",
    "    ]\n",
    "\n",
    "    for input_str, expected in test_cases:\n",
    "        result = parser.parse(input_str)\n",
    "        assert result == expected, f\"Expected {expected}, but got {result}. Input: {input_str}\"\n",
    "        print(f\"Input: {input_str}, Result: {result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import List,Dict,Union\n",
    "\n",
    "from enum import Enum\n",
    "# 分類するラベルのリスト\n",
    "subject_list = [\"Agent01\",\"Agent02\", \"Agent03\", \"Agent04\", \"Agent05\",\n",
    "                   \"Agent06\", \"Agent07\", \"Agent08\", \"Agent09\", \"Agent10\", \n",
    "                   \"Agent11\", \"Agent12\", \"Agent13\", \"Agent14\", \"Agent15\",\"UNSPEC\",\"ANY\"] #TODO:ここ周りテキトーにやってる。ホントは分類ではなく値自身を使えばいいはず\n",
    "verb_list = ['ESTIMATE', 'COMINGOUT', 'DIVINATION', 'GUARD', 'VOTE',\n",
    "            'ATTACK', 'DIVINED', 'IDENTIFIED', 'GUARDED', 'VOTED',\n",
    "            'ATTACKED', 'AGREE', 'DISAGREE', 'Skip', 'Over' ] # REVIEW: Skip, Overをuppercaseにする必要があるかどうか\n",
    "target_list = subject_list\n",
    "species_list = ['HUMAN',\"WEREWOLF\",\"ANY\"]\n",
    "role_list = ['VILLAGER','SEER', 'MEDIUM','BODYGUARD','WEREWOLF','POSSESSED','ANY']\n",
    "\n",
    "protocol_token_list = ['EOS']+ subject_list + verb_list + target_list + species_list + role_list\n",
    "protocol_token_dict = {token: i for i, token in enumerate(protocol_token_list)}\n",
    "\n",
    "#Protocolのラベル用enum\n",
    "Terminal = Enum('Terminal',protocol_token_dict) # TODO:要修正\n",
    "NonTerminal = Enum('NonTerminal',protocol_token_dict) #TODO:要修正\n",
    "\n",
    "terminal_next_gen_rule_list: Dict[NonTerminal,Dict[Terminal,List[Union[NonTerminal,Terminal]]]] = {\"test\":{}} #TODO:要修正\n",
    "non_terminal_first: Dict[NonTerminal,List[Terminal]] = {\"test\":[]} #TODO:要修正\n",
    "next_terminals_in_gen_rule: Dict[NonTerminal,Dict[Terminal,List[List[Terminal]]]] = {\"test\":{}} #TODO:要修正\n",
    "\n",
    "def next_terminals(terminals: List[Terminal]) -> List[Terminal]:\n",
    "    if len(terminals) == 0:\n",
    "        return non_terminal_first[\"sentence\"]\n",
    "\n",
    "    transitions = deque()\n",
    "    cur_non_terminal = \"sentence\" # Start NON TERMINAL\n",
    "    gen_rule = terminal_next_gen_rule_list[cur_non_terminal][terminals[0]]\n",
    "    idx = 0\n",
    "    #構文解析\n",
    "    for t in terminals:\n",
    "        # 非終端記号に到達したので子に移動\n",
    "        while t != gen_rule[idx]:\n",
    "            transitions.push_right((cur_non_terminal,gen_rule,idx+1))\n",
    "            cur_non_terminal = gen_rule[idx]\n",
    "            gen_rule[idx] = terminal_next_gen_rule_list[cur_non_terminal][t]\n",
    "            idx = 0\n",
    "         \n",
    "        idx += 1\n",
    "        # cur_non_terminalの構文解析成功．親に移動\n",
    "        while idx == len(gen_rule) and len(transitions) > 0:\n",
    "            cur_non_terminal,gen_rule,idx = transitions.pop_right()\n",
    "     \n",
    "    #次にとりうる終端記号を列挙\n",
    "    if len(transitions) == 0:# 構文解析が成功し，次にとり得る終端記号がない\n",
    "        return [] #\n",
    "    else:\n",
    "        return next_terminals_in_gen_rule[cur_non_terminal][gen_rule[0]][idx-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First = {}\n",
    "First['sentence'] = ['Skip', 'Over', 'Agent', 'ANY', 'ESTIMATE', 'COMMINGOUT', 'DIVINATION', 'GUARD', 'VOTE', 'ATTACK', 'GUARDED', 'VOTED', 'ATTACKED', 'DIVINED', 'IDENTIFIED', 'AGREE', 'DISAGREE', 'REQUEST', 'INQUIRE', 'NOT', 'BECAUSE', 'XOR', 'AND', 'OR', 'DAY']\n",
    "First['VTR_VT_VTS_AGG_OTS_OS1_OS2_OSS_DAY'] = ['ESTIMATE', 'COMMINGOUT', 'DIVINATION', 'GUARD', 'VOTE', 'ATTACK', 'GUARDED', 'VOTED', 'ATTACKED', 'DIVINED', 'IDENTIFIED', 'AGREE', 'DISAGREE', 'REQUEST', 'INQUIRE', 'NOT', 'BECAUSE', 'XOR', 'AND', 'OR', 'DAY']\n",
    "First['TR'] = ['Agent', 'ANY']\n",
    "First['T'] = ['Agent', 'ANY']\n",
    "First['TSp'] = ['Agent', 'ANY']\n",
    "First['TSe'] = ['Agent', 'ANY']\n",
    "First['S2'] = ['Skip', 'Over', 'Agent', 'ANY', 'ESTIMATE', 'COMMINGOUT', 'DIVINATION', 'GUARD', 'VOTE', 'ATTACK', 'GUARDED', 'VOTED', 'ATTACKED', 'DIVINED', 'IDENTIFIED', 'AGREE', 'DISAGREE', 'REQUEST', 'INQUIRE', 'NOT', 'BECAUSE', 'XOR', 'AND', 'OR', 'DAY']\n",
    "First['SS'] = ['Skip', 'Over', 'Agent', 'ANY', 'ESTIMATE', 'COMMINGOUT', 'DIVINATION', 'GUARD', 'VOTE', 'ATTACK', 'GUARDED', 'VOTED', 'ATTACKED', 'DIVINED', 'IDENTIFIED', 'AGREE', 'DISAGREE', 'REQUEST', 'INQUIRE', 'NOT', 'BECAUSE', 'XOR', 'AND', 'OR', 'DAY']\n",
    "First['recsentence'] = ['Skip', 'Over', 'Agent', 'ANY', 'ESTIMATE', 'COMMINGOUT', 'DIVINATION', 'GUARD', 'VOTE', 'ATTACK', 'GUARDED', 'VOTED', 'ATTACKED', 'DIVINED', 'IDENTIFIED', 'AGREE', 'DISAGREE', 'REQUEST', 'INQUIRE', 'NOT', 'BECAUSE', 'XOR', 'AND', 'OR', 'DAY']\n",
    "First['rec2sentence'] = ['Skip', 'Over', 'Agent', 'ANY', 'ESTIMATE', 'COMMINGOUT', 'DIVINATION', 'GUARD', 'VOTE', 'ATTACK', 'GUARDED', 'VOTED', 'ATTACKED', 'DIVINED', 'IDENTIFIED', 'AGREE', 'DISAGREE', 'REQUEST', 'INQUIRE', 'NOT', 'BECAUSE', 'XOR', 'AND', 'OR', 'DAY', 'r']\n",
    "First['species'] = ['HUMAN', 'WEREWOLF', 'ANY']\n",
    "First['role'] = ['VILLAGER', 'SEER', 'MEDIUM', 'BODYGUARD', 'WEREWOLF', 'POSSESSED']\n",
    "First['talk_number'] = ['day']\n",
    "First['agent_number'] = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n",
    "First['day_number'] = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "First['ID_number'] = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Follow = {}\n",
    "Follow['sentence'] = ['r']\n",
    "Follow['VTR_VT_VTS_AGG_OTS_OS1_OS2_OSS_DAY'] = ['r']\n",
    "Follow['TR'] = ['r']\n",
    "Follow['T'] = ['r']\n",
    "Follow['TSp'] = ['r']\n",
    "Follow['TSe'] = ['r']\n",
    "Follow['S2'] = ['r']\n",
    "Follow['SS'] = ['r']\n",
    "Follow['recsentence'] = ['r']\n",
    "Follow['rec2sentence'] = ['r']\n",
    "Follow['species'] = ['r']\n",
    "Follow['role'] = ['r']\n",
    "Follow['talk_number'] = ['r']\n",
    "Follow['agent_number'] = ['ESTIMATE', 'COMMINGOUT', 'DIVINATION', 'GUARD', 'VOTE', 'ATTACK', 'GUARDED', 'VOTED', 'ATTACKED', 'DIVINED', 'IDENTIFIED', 'AGREE', 'DISAGREE', 'REQUEST', 'INQUIRE', 'NOT', 'BECAUSE', 'XOR', 'AND', 'OR', 'DAY', 'VILLAGER', 'SEER', 'MEDIUM', 'BODYGUARD', 'WEREWOLF', 'POSSESSED', 'r', 'HUMAN', 'ANY', 'l']\n",
    "Follow['day_number'] = ['l', 'IDcol']\n",
    "Follow['ID_number'] = ['r']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LL1Grammar:\n",
    "    def __init__(self, non_terminals, terminals, start_symbol, production_rules):\n",
    "        self.non_terminals = non_terminals\n",
    "        self.terminals = terminals\n",
    "        self.start_symbol = start_symbol\n",
    "        self.production_rules = production_rules\n",
    "        self.first_sets = self.compute_first_sets()\n",
    "        self.follow_sets = self.compute_follow_sets()\n",
    "        self.parse_table = self.construct_parse_table()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"LL1Grammar(non_terminals={self.non_terminals}, terminals={self.terminals}, start_symbol={self.start_symbol}, production_rules={self.production_rules})\"\n",
    "    \n",
    "    def add_production_rule(self, non_terminal, production):\n",
    "        if non_terminal in self.non_terminals:\n",
    "            self.production_rules[non_terminal].append(production)\n",
    "        else:\n",
    "            raise ValueError(f\"Non-terminal '{non_terminal}' not found in grammar.\")\n",
    "\n",
    "    def remove_production_rule(self, non_terminal, production):\n",
    "        if non_terminal in self.non_terminals:\n",
    "            if production in self.production_rules[non_terminal]:\n",
    "                self.production_rules[non_terminal].remove(production)\n",
    "            else:\n",
    "                raise ValueError(f\"Production '{production}' not found for non-terminal '{non_terminal}'.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Non-terminal '{non_terminal}' not found in grammar.\")\n",
    "        \n",
    "    def compute_first_sets(self):\n",
    "        first_sets = {nt: set() for nt in self.non_terminals}\n",
    "        change = True\n",
    "\n",
    "        while change:\n",
    "            change = False\n",
    "            for nt in self.non_terminals:\n",
    "                for prod in self.production_rules[nt]:\n",
    "                    if prod[0] in self.terminals:\n",
    "                        if prod[0] not in first_sets[nt]:\n",
    "                            first_sets[nt].add(prod[0])\n",
    "                            change = True\n",
    "                    else:\n",
    "                        for symbol in prod:\n",
    "                            if symbol in self.terminals:\n",
    "                                break\n",
    "                            for first in first_sets[symbol]:\n",
    "                                if first not in first_sets[nt]:\n",
    "                                    first_sets[nt].add(first)\n",
    "                                    change = True\n",
    "                            if 'ε' not in first_sets[symbol]:\n",
    "                                break\n",
    "        return first_sets\n",
    "\n",
    "    def compute_follow_sets(self):\n",
    "        follow_sets = {nt: set() for nt in self.non_terminals}\n",
    "        follow_sets[self.start_symbol].add('$')\n",
    "\n",
    "        change = True\n",
    "        while change:\n",
    "            change = False\n",
    "            for nt in self.non_terminals:\n",
    "                for prod in self.production_rules[nt]:\n",
    "                    for i, symbol in enumerate(prod[:-1]):\n",
    "                        if symbol in self.non_terminals:\n",
    "                            for next_sym in prod[i + 1:]:\n",
    "                                if next_sym in self.terminals:\n",
    "                                    if next_sym not in follow_sets[symbol]:\n",
    "                                        follow_sets[symbol].add(next_sym)\n",
    "                                        change = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    for first in self.first_sets[next_sym]:\n",
    "                                        if first not in follow_sets[symbol] and first != 'ε':\n",
    "                                            follow_sets[symbol].add(first)\n",
    "                                            change = True\n",
    "                                    if 'ε' not in self.first_sets[next_sym]:\n",
    "                                        break\n",
    "                            else:\n",
    "                                for follow in follow_sets[nt]:\n",
    "                                    if follow not in follow_sets[symbol]:\n",
    "                                        follow_sets[symbol].add(follow)\n",
    "                                        change = True\n",
    "        return follow_sets\n",
    "\n",
    "    def construct_parse_table(self):\n",
    "        parse_table = {nt: {t: '' for t in self.terminals} for nt in self.non_terminals}\n",
    "        parse_table.update({nt: {t: '' for t in self.terminals} for nt in ['$']})\n",
    "\n",
    "        for nt, prods in self.production_rules.items():\n",
    "            for prod in prods:\n",
    "                first_symbols = set()\n",
    "                for symbol in prod:\n",
    "                    if symbol in self.terminals:\n",
    "                        first_symbols.add(symbol)\n",
    "                        break\n",
    "                    else:\n",
    "                        first_symbols |= self.first_sets[symbol]\n",
    "                        if 'ε' not in first_symbols:\n",
    "                            break\n",
    "                        first_symbols -= {'ε'}\n",
    "\n",
    "                for terminal in first_symbols:\n",
    "                    parse_table[nt][terminal] = prod\n",
    "\n",
    "                if 'ε' in first_symbols:\n",
    "                    for terminal in self.follow_sets[nt]:\n",
    "                        if terminal != '$':\n",
    "                            parse_table[nt][terminal] = prod\n",
    "\n",
    "        return parse_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 以下は、前回の例で使用した文法に基づいてLL(1)文法のインスタンスを作成するコードです。\n",
    "non_terminals = {'S', 'A', 'B'}\n",
    "terminals = {'a', 'b', 'c', 'd', 'ε'}\n",
    "start_symbol = 'S'\n",
    "production_rules = {\n",
    "    'S': ['AB'],\n",
    "    'A': ['aA', 'b', 'ε'],\n",
    "    'B': ['cB', 'd', 'ε']\n",
    "}\n",
    "\n",
    "grammar = LL1Grammar(non_terminals, terminals, start_symbol, production_rules)\n",
    "for key, value in grammar.parse_table.items():\n",
    "    print(key, value)\n",
    "#print(grammar.parse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Set\n",
    "\n",
    "class LL1Grammar:\n",
    "    def __init__(self, non_terminals: Set[str], terminals: Set[str], start_symbol: str, production_rules: Dict[str, List[str]]):\n",
    "        self.non_terminals = non_terminals\n",
    "        self.terminals = terminals\n",
    "        self.start_symbol = start_symbol\n",
    "        self.production_rules = production_rules\n",
    "        self.first_sets = self.compute_first_sets()\n",
    "        self.follow_sets = self.compute_follow_sets()\n",
    "        self.parse_table = self.construct_parse_table()\n",
    "\n",
    "    # ... (Other methods and __repr__)\n",
    "    def __repr__(self):\n",
    "        return f\"LL1Grammar(non_terminals={self.non_terminals}, terminals={self.terminals}, start_symbol={self.start_symbol}, production_rules={self.production_rules})\"\n",
    "    \n",
    "    def add_production_rule(self, non_terminal, production):\n",
    "        if non_terminal in self.non_terminals:\n",
    "            self.production_rules[non_terminal].append(production)\n",
    "        else:\n",
    "            raise ValueError(f\"Non-terminal '{non_terminal}' not found in grammar.\")\n",
    "\n",
    "    def remove_production_rule(self, non_terminal, production):\n",
    "        if non_terminal in self.non_terminals:\n",
    "            if production in self.production_rules[non_terminal]:\n",
    "                self.production_rules[non_terminal].remove(production)\n",
    "            else:\n",
    "                raise ValueError(f\"Production '{production}' not found for non-terminal '{non_terminal}'.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Non-terminal '{non_terminal}' not found in grammar.\")\n",
    "\n",
    "    def compute_first_sets(self) -> Dict[str, Set[str]]:\n",
    "        first_sets = {nt: set() for nt in self.non_terminals}\n",
    "        change = True\n",
    "\n",
    "        while change:\n",
    "            change = False\n",
    "            for nt in self.non_terminals:\n",
    "                for prod in self.production_rules[nt]:\n",
    "                    prod_symbols = prod.split()  # Update here\n",
    "                    if prod_symbols[0] in self.terminals:\n",
    "                        if prod_symbols[0] not in first_sets[nt]:\n",
    "                            first_sets[nt].add(prod_symbols[0])\n",
    "                            change = True\n",
    "                    else:\n",
    "                        for symbol in prod_symbols:\n",
    "                            if symbol in self.terminals:\n",
    "                                break\n",
    "                            for first in first_sets[symbol]:\n",
    "                                if first not in first_sets[nt]:\n",
    "                                    first_sets[nt].add(first)\n",
    "                                    change = True\n",
    "                            if 'ε' not in first_sets[symbol]:\n",
    "                                break\n",
    "        return first_sets\n",
    "\n",
    "    def compute_follow_sets(self) -> Dict[str, Set[str]]:\n",
    "        follow_sets = {nt: set() for nt in self.non_terminals}\n",
    "        follow_sets[self.start_symbol].add('$')\n",
    "\n",
    "        change = True\n",
    "        while change:\n",
    "            change = False\n",
    "            for nt in self.non_terminals:\n",
    "                for prod in self.production_rules[nt]:\n",
    "                    prod_symbols = prod.split()  # Update here\n",
    "                    for i, symbol in enumerate(prod_symbols[:-1]):\n",
    "                        if symbol in self.non_terminals:\n",
    "                            for next_sym in prod_symbols[i + 1:]:\n",
    "                                if next_sym in self.terminals:\n",
    "                                    if next_sym not in follow_sets[symbol]:\n",
    "                                        follow_sets[symbol].add(next_sym)\n",
    "                                        change = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    for first in self.first_sets[next_sym]:\n",
    "                                        if first not in follow_sets[symbol] and first != 'ε':\n",
    "                                            follow_sets[symbol].add(first)\n",
    "                                            change = True\n",
    "                                    if 'ε' not in self.first_sets[next_sym]:\n",
    "                                        break\n",
    "                            else:\n",
    "                                for follow in follow_sets[nt]:\n",
    "                                    if follow not in follow_sets[symbol]:\n",
    "                                        follow_sets[symbol].add(follow)\n",
    "                                        change = True\n",
    "        return follow_sets\n",
    "\n",
    "    def construct_parse_table(self):\n",
    "        parse_table = {nt: {t: '' for t in self.terminals} for nt in self.non_terminals}\n",
    "        parse_table.update({nt: {t: '' for t in self.terminals} for nt in ['$']})\n",
    "\n",
    "        for nt, prods in self.production_rules.items():\n",
    "            for prod in prods:\n",
    "                prod_symbols = prod.split()  # Update here\n",
    "                first_symbols = set()\n",
    "                for symbol in prod_symbols:\n",
    "                    if symbol in self.terminals:\n",
    "                        first_symbols.add(symbol)\n",
    "                        break\n",
    "                    else:\n",
    "                        first_symbols |= self.first_sets[symbol]\n",
    "                        if 'ε' not in first_symbols:\n",
    "                            break\n",
    "                first_symbols -= {'ε'}\n",
    "\n",
    "                for terminal in first_symbols:\n",
    "                    parse_table[nt][terminal] = prod\n",
    "\n",
    "                if 'ε' in first_symbols:\n",
    "                    for terminal in self.follow_sets[nt]:\n",
    "                        if terminal != '$':\n",
    "                            parse_table[nt][terminal] = prod\n",
    "\n",
    "        return parse_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_terminals = {'S', 'A', 'B'}\n",
    "terminals = {'a', 'b', 'c', 'd', 'ε'}\n",
    "start_symbol = 'S'\n",
    "production_rules = {\n",
    "    'S': ['A B'],\n",
    "    'A': ['a A', 'b', 'ε'],\n",
    "    'B': ['c B', 'd', 'ε']\n",
    "}\n",
    "\n",
    "grammar = LL1Grammar(non_terminals, terminals, start_symbol, production_rules)\n",
    "for key, value in grammar.parse_table.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_day(s):\n",
    "    # \"day[数字]\" を \"day [各数字]\" に変換する正規表現\n",
    "    pattern = r\"(day)(\\d+)\"\n",
    "\n",
    "    # 文字列内のパターンにマッチする部分を変換する関数\n",
    "    def replace_func(match):\n",
    "        day_str = match.group(1)\n",
    "        digits = match.group(2)\n",
    "\n",
    "        # 数字を1桁ずつスペースで区切る\n",
    "        spaced_digits = ' '.join(list(digits))\n",
    "\n",
    "        return f\"{day_str} {spaced_digits}\"\n",
    "\n",
    "    # 文字列内のパターンにマッチする部分を変換\n",
    "    s = re.sub(pattern, replace_func, s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# 使用例\n",
    "input_str = \"Today is day10 and tomorrow is day32.\"\n",
    "output_str = convert_day(input_str)\n",
    "print(output_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def revert_day(s):\n",
    "    # \"day [各数字]\" を \"day[数字]\" に変換する正規表現\n",
    "    pattern = r\"(day)(?:\\s+(\\d))+\"\n",
    "\n",
    "    # 文字列内のパターンにマッチする部分を変換する関数\n",
    "    def replace_func(match):\n",
    "        day_str = match.group(1)\n",
    "        digits = match.group(2)\n",
    "\n",
    "        # スペースで区切られた数字を連結\n",
    "        concatenated_digits = ''.join(match.groups()[1:])\n",
    "\n",
    "        return f\"{day_str}{concatenated_digits}\"\n",
    "\n",
    "    # 文字列内のパターンにマッチする部分を変換\n",
    "    s = re.sub(pattern, replace_func, s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# 使用例\n",
    "input_str = \"Today is day 1 0 and tomorrow is day 3 2.\"\n",
    "output_str = revert_day(input_str)\n",
    "print(output_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def add_brackets(s):\n",
    "    # \"Agent[数字]\" を \"Agent [数字]\" に変換する正規表現\n",
    "    pattern = r\"(Agent) (\\d+)\"\n",
    "\n",
    "    # 文字列内のパターンにマッチする部分を変換する関数\n",
    "    agent_str =\"\"\n",
    "    digits = \"\"\n",
    "    def replace_func(match):\n",
    "        agent_str = match.group(1)\n",
    "        digits = match.group(2)\n",
    "\n",
    "        return f\"{agent_str}[{digits}]\"\n",
    "    print(agent_str, digits)\n",
    "\n",
    "    # 文字列内のパターンにマッチする部分を変換\n",
    "    s = re.sub(pattern, replace_func, s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# 使用例\n",
    "input_str = \"This is Agent 07 and that is Agent 11.\"\n",
    "output_str = add_brackets(input_str)\n",
    "print(output_str)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Tokens test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, LogitsProcessorList, LogitsProcessor\n",
    "\n",
    "# 日本語プロトコル変換用\n",
    "from aiwolfk2b.agentLPS.jp_to_protocol_converter import JPToProtocolConverter\n",
    "from aiwolfk2b.utils.ll1_grammar import LL1Grammar, aiwolf_protocol_grammar, convert_ll1_to_protocol,convert_protocol_to_ll1\n",
    "from typing import Any, Callable, Dict, List,Optional, Tuple, Union\n",
    "\n",
    "\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 文法に従ったトークンのみを生成するlogits_processor\n",
    "class ConstrainedLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, grammar: LL1Grammar, tokenizer: T5Tokenizer):\n",
    "        super().__init__()\n",
    "        self.grammar:LL1Grammar = grammar\n",
    "        self.tokenizer:T5Tokenizer = tokenizer\n",
    "        # self.space_token_id = self.tokenizer.convert_tokens_to_ids(\" \")\n",
    "        # print(\"space_token_id:\",self.space_token_id)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        #print(input_ids)\n",
    "        #一度文章に変換\n",
    "        protocol_batch = self.tokenizer.batch_decode(input_ids,skip_special_tokens=True)\n",
    "        valid_tokens_ids = []\n",
    "        agent_numbers_ids_1_15 = {2072, 1905, 2078, 2458, 2357, 2195, 2227, 2246, 2224,333,359,350,491,506,423}\n",
    "        numbers_ids_0_9 = {942, 291, 293, 294, 306, 320, 331, 334, 337, 341}\n",
    "        #各文章をスペースで分割\n",
    "        for idx,protocol in enumerate(protocol_batch):\n",
    "            possible_tokens_ids = set()\n",
    "            #protocol_split = protocol.split()\n",
    "            #空の場合\n",
    "            print(\"protocol:\",protocol)\n",
    "            print(\"protocol_ids:\",input_ids[idx])\n",
    "            if protocol == \"\":\n",
    "                #最初に来るトークンの候補を取得\n",
    "                possible_terminals = self.grammar.first_sets[self.grammar.start_symbol]\n",
    "                # print(\"possible_terminals:\",possible_terminals)\n",
    "                for terminal in possible_terminals:\n",
    "                    terminal_token_id = self.tokenizer.convert_tokens_to_ids(terminal)\n",
    "                    possible_tokens_ids.add(terminal_token_id)\n",
    "            #最後のトークンが\"day\"の場合、次にありえるトークンを設定\n",
    "            elif input_ids[idx][-1] == 2726: # \"day\" <-> 2726\n",
    "                possible_tokens_ids.add(262)#\" \" <-> 262\n",
    "            #最後のトークンが01~15の場合、次にありえるトークンを設定\n",
    "            elif input_ids[idx][-1] in agent_numbers_ids_1_15:\n",
    "                possible_tokens_ids.add(262)#\" \" <-> 262\n",
    "            #最後のトークンが01~09の場合、次にありえるトークンを設定\n",
    "            elif input_ids[idx][-1] in numbers_ids_0_9:\n",
    "                possible_tokens_ids.add(262)#\" \" <-> 262\n",
    "            \n",
    "            # #最後のトークンが262の場合、次にありえるトークンを設定\n",
    "            # elif input_ids[idx][-1] == 262:\n",
    "            #     #01~09, )が該当\n",
    "            #     after262_tokens = [1905, 2078, 2224, 2357, 2458, 2246, 2072, 2227, 2195, 268]\n",
    "            #     possible_terminals = self.grammar.get_next_terminals(protocol)\n",
    "            #     possible_terminals_tokens = set()\n",
    "            #     for terminal in possible_terminals:\n",
    "            #         possible_terminals_tokens.add(self.tokenizer.convert_tokens_to_ids(terminal))\n",
    "            #     possible_tokens_ids.add(after262_tokens & possible_terminals)\n",
    "            #最後のトークンが262以外の場合は、次に来る終端記号がそのまま続く\n",
    "            \n",
    "            #次に来る終端記号がそのまま続く\n",
    "            else:    \n",
    "                possible_terminals = self.grammar.get_next_terminals(protocol)\n",
    "                # print(\"possible_terminals:\",possible_terminals)\n",
    "                for terminal in possible_terminals:\n",
    "                    terminal_token_id = self.tokenizer.convert_tokens_to_ids(terminal)\n",
    "                    if terminal != \"(\":\n",
    "                        possible_tokens_ids.add(terminal_token_id)\n",
    "                    elif terminal == \")\":\n",
    "                        if input_ids[idx][-1] == 262: #すでに　空白(262)がある場合\n",
    "                            possible_tokens_ids.add(268)\n",
    "                        else:\n",
    "                            possible_tokens_ids.add(262) #\"number )\"を実現するために、262を追加\n",
    "                    elif terminal == \"ε\":\n",
    "                        possible_tokens_ids.add(self.tokenizer.eos_token_id)\n",
    "                    else:\n",
    "                        possible_tokens_ids.add(290)\n",
    "                    \n",
    "                    \n",
    "                            \n",
    "            #もし、得られる終端記号の候補がない場合は、終了記号を追加\n",
    "            if len(possible_tokens_ids) == 0:\n",
    "                possible_tokens_ids.add(self.tokenizer.eos_token_id)\n",
    "                \n",
    "            valid_tokens_ids.append(possible_tokens_ids)\n",
    "\n",
    "        #print(\"eos_token_id:{}\".format(self.tokenizer.eos_token_id))\n",
    "        print(\"valid_token_ids:{}\".format(valid_tokens_ids))\n",
    "        # print(\"scores.shape:{}\".format(scores.shape))\n",
    "\n",
    "        for batch_idx in range(scores.shape[0]):\n",
    "            for token_id in range(scores.shape[1]):\n",
    "                if token_id not in valid_tokens_ids[batch_idx]:\n",
    "                    scores[batch_idx, token_id] = float('-inf')\n",
    "\n",
    "        return scores\n",
    "\n",
    "class T5JPToProtocolConverter(JPToProtocolConverter):\n",
    "    def __init__(self, model_name: str = \"default\", model_path: str = \"default\"):\n",
    "        #モデルの読み込み\n",
    "        if model_name == \"default\":\n",
    "            MODEL_NAME = \"sonoisa/t5-base-english-japanese\"\n",
    "        else:\n",
    "            MODEL_NAME = model_name\n",
    "        if model_path == \"default\":\n",
    "            MODEL_PATH = \"/home/takuya/HDD1/work/AI_Wolf/2023S_AIWolfK2B/aiwolfk2b/agentLPS/jp2protocol_model/t5_upper_20230514_4.pth\"\n",
    "        else:\n",
    "            MODEL_PATH = model_path\n",
    "            \n",
    "        self.model_name = MODEL_NAME\n",
    "        self.model_path = MODEL_PATH\n",
    "        \n",
    "        self.model:T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "        self.tokenizer:T5Tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        # プロトコル変換用に文法の準備\n",
    "        self.protocol_grammar: LL1Grammar = aiwolf_protocol_grammar()\n",
    "        # プロトコル変換用のlogits_processorの準備\n",
    "        self.logits_processor = LogitsProcessorList([\n",
    "            ConstrainedLogitsProcessor(self.protocol_grammar, self.tokenizer)\n",
    "        ])\n",
    "        #トークナイザーにLL1文法の終端記号を追加し、モデルのトークナイザーをリサイズ\n",
    "        # terminals_lower = [terminal.lower() for terminal in self.protocol_grammar.terminals]\n",
    "        # terminals_lower = set(terminals_lower)\n",
    "        # new_tokens = terminals_lower - set(self.tokenizer.get_vocab().keys())\n",
    "        # self.tokenizer.add_tokens(list(new_tokens))\n",
    "        # self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        #学習したモデルがあれば読み込む\n",
    "        if MODEL_PATH != \"\":\n",
    "            self.model.load_state_dict(torch.load(MODEL_PATH,map_location=torch.device(device)))\n",
    "        \n",
    "    def convert(self, text_list: List[str]) -> List[str]:\n",
    "        input = self.tokenizer.batch_encode_plus(text_list, max_length=128, padding='max_length', return_tensors='pt', truncation=True)\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "        inputs=input[\"input_ids\"],\n",
    "        attention_mask=input[\"attention_mask\"],\n",
    "        #force_words_ids=self.force_words_ids,\n",
    "        num_beams=5,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=0,\n",
    "        remove_invalid_values=True,\n",
    "        logits_processor=self.logits_processor,\n",
    "        max_length = 16,\n",
    "        early_stopping=True,\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_test_T5JPToProtocolConverter(converter):\n",
    "    text = \"Agent[08]が襲われたAgent[05]を霊媒すると人間だった\"\n",
    "    # 入力する文章\n",
    "    text_list = [\n",
    "        \"Agent[03]はAgent[08]が狼だと推測する\",\n",
    "        # \"Agent[06]はAgent[06]が占い師だとカミングアウトする\",\n",
    "        # \"Agent[12]が占った結果Agent[10]は人狼だった\",\n",
    "        # \"Agent[12]が占った結果Agent[10]は人間だった\",\n",
    "        # \"Agent[08]が襲われたAgent[05]を霊媒すると人間だった\",\n",
    "        # \"Agent[05]はAgent[10]を護衛した\",\n",
    "        #  \"Agent[10]はAgent[12]に投票する\",\n",
    "        # \"Agent[06]はAgent[08]が狼だと思う\",\n",
    "        # \"私が占い師です\",\n",
    "        # \"Agent[12]が占った結果、Agent[10]は人狼でした\",\n",
    "        # \"Agent[12]が占った結果、Agent[10]は人間でした\",\n",
    "        # \"Agent[12]がAgent[05]を霊媒すると人間でした\",\n",
    "        # \"Agent[12]はAgent[10]を守った\",\n",
    "        # \"Agent[10]はAgent[12]に投票します\",\n",
    "        # \"Agent[08]が狼だと思う\",\n",
    "        # \"私が占い師です\",\n",
    "        # \"占った結果、Agent[10]は人狼でした\",\n",
    "        # \"占った結果、Agent[10]は人間でした\",\n",
    "        # \"Agent[05]を霊媒すると人間でした\",\n",
    "        # \"私はAgent[10]を守った\",\n",
    "        # \"私はAgent[12]に投票します\",\n",
    "    ]\n",
    "    protocol = converter.convert([text])\n",
    "    print(\"one text:\",protocol)\n",
    "    \n",
    "    protocols = converter.convert(text_list)\n",
    "    print(\"text_list:\", protocols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for T5ForConditionalGeneration:\n\tsize mismatch for shared.weight: copying a param with shape torch.Size([32133, 768]) from checkpoint, the shape in current model is torch.Size([32123, 768]).\n\tsize mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([32133, 768]) from checkpoint, the shape in current model is torch.Size([32123, 768]).\n\tsize mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([32133, 768]) from checkpoint, the shape in current model is torch.Size([32123, 768]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32133, 768]) from checkpoint, the shape in current model is torch.Size([32123, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m converter \u001b[39m=\u001b[39m T5JPToProtocolConverter()\n",
      "Cell \u001b[0;32mIn[2], line 121\u001b[0m, in \u001b[0;36mT5JPToProtocolConverter.__init__\u001b[0;34m(self, model_name, model_path)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39m#学習したモデルがあれば読み込む\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m MODEL_PATH \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(MODEL_PATH,map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(device)))\n",
      "File \u001b[0;32m~/HDD1/work/AI_Wolf/2023S_AIWolfK2B/env_k2b/lib/python3.8/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for T5ForConditionalGeneration:\n\tsize mismatch for shared.weight: copying a param with shape torch.Size([32133, 768]) from checkpoint, the shape in current model is torch.Size([32123, 768]).\n\tsize mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([32133, 768]) from checkpoint, the shape in current model is torch.Size([32123, 768]).\n\tsize mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([32133, 768]) from checkpoint, the shape in current model is torch.Size([32123, 768]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32133, 768]) from checkpoint, the shape in current model is torch.Size([32123, 768])."
     ]
    }
   ],
   "source": [
    "converter = T5JPToProtocolConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "valid_token_ids:[{32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}, {32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}, {32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}, {32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}, {32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}]\n",
      "protocol: COMINGOUT\n",
      "protocol_ids: tensor([    0, 32129])\n",
      "protocol: COMINGOUT\n",
      "protocol_ids: tensor([    0, 32129])\n",
      "protocol: COMINGOUT\n",
      "protocol_ids: tensor([    0, 32129])\n",
      "protocol: COMINGOUT\n",
      "protocol_ids: tensor([    0, 32129])\n",
      "protocol: NOT\n",
      "protocol_ids: tensor([    0, 32128])\n",
      "valid_token_ids:[{32114, 32122}, {32114, 32122}, {32114, 32122}, {32114, 32122}, {290}]\n",
      "protocol: COMINGOUT Agent\n",
      "protocol_ids: tensor([    0, 32129, 32122])\n",
      "protocol: COMINGOUT Agent\n",
      "protocol_ids: tensor([    0, 32129, 32122])\n",
      "protocol: COMINGOUT Agent\n",
      "protocol_ids: tensor([    0, 32129, 32122])\n",
      "protocol: COMINGOUT Agent\n",
      "protocol_ids: tensor([    0, 32129, 32122])\n",
      "protocol: COMINGOUT ANY\n",
      "protocol_ids: tensor([    0, 32129, 32114])\n",
      "valid_token_ids:[{2246, 423, 359, 491, 333, 2458, 2224, 1905, 2227, 2195, 2357, 2078, 2072, 506, 350}, {2246, 423, 359, 491, 333, 2458, 2224, 1905, 2227, 2195, 2357, 2078, 2072, 506, 350}, {2246, 423, 359, 491, 333, 2458, 2224, 1905, 2227, 2195, 2357, 2078, 2072, 506, 350}, {2246, 423, 359, 491, 333, 2458, 2224, 1905, 2227, 2195, 2357, 2078, 2072, 506, 350}, {32102, 32104, 32106, 32124, 32126, 32127}]\n",
      "protocol: COMINGOUT Agent 05\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2072])\n",
      "protocol: COMINGOUT Agent 05\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2072])\n",
      "protocol: COMINGOUT Agent 05\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2072])\n",
      "protocol: COMINGOUT Agent 05\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2072])\n",
      "protocol: COMINGOUT Agent 04\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2458])\n",
      "valid_token_ids:[{32102, 32104, 32106, 32124, 32126, 32127}, {32102, 32104, 32106, 32124, 32126, 32127}, {32102, 32104, 32106, 32124, 32126, 32127}, {32102, 32104, 32106, 32124, 32126, 32127}, {32102, 32104, 32106, 32124, 32126, 32127}]\n",
      "protocol: COMINGOUT Agent 05 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2072, 32124])\n",
      "protocol: COMINGOUT Agent 05 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2072, 32124])\n",
      "protocol: COMINGOUT Agent 05 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2072, 32124])\n",
      "protocol: COMINGOUT Agent 05 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2072, 32124])\n",
      "protocol: COMINGOUT Agent 04 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2458, 32124])\n",
      "valid_token_ids:[{1}, {1}, {1}, {1}, {1}]\n",
      "one text: ['COMINGOUT Agent 05 POSSESSED']\n",
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "protocol: \n",
      "protocol_ids: tensor([0])\n",
      "valid_token_ids:[{32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}, {32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}, {32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}, {32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}, {32128, 32129, 32130, 32131, 32132, 32100, 32101, 32103, 32105, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32116, 32117, 32118, 32119, 32120, 32122, 32123, 32125}]\n",
      "protocol: COMINGOUT\n",
      "protocol_ids: tensor([    0, 32129])\n",
      "protocol: COMINGOUT\n",
      "protocol_ids: tensor([    0, 32129])\n",
      "protocol: COMINGOUT\n",
      "protocol_ids: tensor([    0, 32129])\n",
      "protocol: COMINGOUT\n",
      "protocol_ids: tensor([    0, 32129])\n",
      "protocol: COMINGOUT\n",
      "protocol_ids: tensor([    0, 32129])\n",
      "valid_token_ids:[{32114, 32122}, {32114, 32122}, {32114, 32122}, {32114, 32122}, {32114, 32122}]\n",
      "protocol: COMINGOUT Agent\n",
      "protocol_ids: tensor([    0, 32129, 32122])\n",
      "protocol: COMINGOUT Agent\n",
      "protocol_ids: tensor([    0, 32129, 32122])\n",
      "protocol: COMINGOUT Agent\n",
      "protocol_ids: tensor([    0, 32129, 32122])\n",
      "protocol: COMINGOUT Agent\n",
      "protocol_ids: tensor([    0, 32129, 32122])\n",
      "protocol: COMINGOUT Agent\n",
      "protocol_ids: tensor([    0, 32129, 32122])\n",
      "valid_token_ids:[{2246, 423, 359, 491, 333, 2458, 2224, 1905, 2227, 2195, 2357, 2078, 2072, 506, 350}, {2246, 423, 359, 491, 333, 2458, 2224, 1905, 2227, 2195, 2357, 2078, 2072, 506, 350}, {2246, 423, 359, 491, 333, 2458, 2224, 1905, 2227, 2195, 2357, 2078, 2072, 506, 350}, {2246, 423, 359, 491, 333, 2458, 2224, 1905, 2227, 2195, 2357, 2078, 2072, 506, 350}, {2246, 423, 359, 491, 333, 2458, 2224, 1905, 2227, 2195, 2357, 2078, 2072, 506, 350}]\n",
      "protocol: COMINGOUT Agent 08\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2078])\n",
      "protocol: COMINGOUT Agent 08\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2078])\n",
      "protocol: COMINGOUT Agent 08\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2078])\n",
      "protocol: COMINGOUT Agent 08\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2078])\n",
      "protocol: COMINGOUT Agent 06\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2224])\n",
      "valid_token_ids:[{32102, 32104, 32106, 32124, 32126, 32127}, {32102, 32104, 32106, 32124, 32126, 32127}, {32102, 32104, 32106, 32124, 32126, 32127}, {32102, 32104, 32106, 32124, 32126, 32127}, {32102, 32104, 32106, 32124, 32126, 32127}]\n",
      "protocol: COMINGOUT Agent 06 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2224, 32124])\n",
      "protocol: COMINGOUT Agent 08 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2078, 32124])\n",
      "protocol: COMINGOUT Agent 08 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2078, 32124])\n",
      "protocol: COMINGOUT Agent 08 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2078, 32124])\n",
      "protocol: COMINGOUT Agent 08 POSSESSED\n",
      "protocol_ids: tensor([    0, 32129, 32122,  2078, 32124])\n",
      "valid_token_ids:[{1}, {1}, {1}, {1}, {1}]\n",
      "text_list: ['COMINGOUT Agent 06 POSSESSED']\n"
     ]
    }
   ],
   "source": [
    "unit_test_T5JPToProtocolConverter(converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = T5JPToProtocolConverter(model_path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_protocols = [\n",
    "    \"INQUIRE Agent 02 ( Agent 03 DISAGREE day 2 ID 2 12 )\",\n",
    "    \"Agent 01 ESTIMATE ANY POSSESSED\",\n",
    "    \"REQUEST\",\n",
    "    \"INQUIRE Agent\"]\n",
    "for protocol in test_protocols:\n",
    "    token_ids = converter.tokenizer.encode(protocol)\n",
    "    print(\"protocol:{} \\n token_ids:{}\\n\".format(protocol,token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminal:divination \t tokens:[1048, 3971, 799, 1]\n",
      "terminal:comingout \t tokens:[8602, 2845, 1]\n",
      "terminal:werewolf \t tokens:[400, 29949, 1]\n",
      "terminal:possessed \t tokens:[23501, 312, 1]\n",
      "terminal:estimate \t tokens:[262, 29475, 1]\n",
      "terminal:08 \t tokens:[262, 2078, 1]\n",
      "terminal:seer \t tokens:[1458, 358, 1]\n",
      "terminal:01 \t tokens:[262, 1905, 1]\n",
      "terminal:villager \t tokens:[1725, 370, 1]\n",
      "terminal:skip \t tokens:[7528, 408, 1]\n",
      "terminal:02 \t tokens:[262, 2246, 1]\n",
      "terminal:03 \t tokens:[262, 2227, 1]\n",
      "terminal:09 \t tokens:[262, 2195, 1]\n",
      "terminal:xor \t tokens:[1047, 614, 1]\n",
      "terminal:bodyguard \t tokens:[3724, 18339, 1]\n",
      "terminal:05 \t tokens:[262, 2072, 1]\n",
      "terminal:) \t tokens:[262, 268, 1]\n",
      "terminal:guarded \t tokens:[7218, 312, 1]\n",
      "terminal:07 \t tokens:[262, 2357, 1]\n",
      "terminal:divined \t tokens:[28316, 321, 1]\n",
      "terminal:06 \t tokens:[262, 2224, 1]\n",
      "terminal:inquire \t tokens:[279, 3144, 530, 1]\n",
      "terminal:04 \t tokens:[262, 2458, 1]\n"
     ]
    }
   ],
   "source": [
    "#語彙のうち、1 token idで表現されていないものを調べる\n",
    "terminals_lower = [terminal.lower() for terminal in converter.protocol_grammar.terminals]\n",
    "terminals_lower = set(terminals_lower)\n",
    "for terminal in terminals_lower:\n",
    "    tokens = converter.tokenizer.encode(terminal)\n",
    "    if len(tokens) > 2:\n",
    "        print(\"terminal:{} \\t tokens:{}\".format(terminal,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_tokens: set()\n"
     ]
    }
   ],
   "source": [
    "#新たに追加されるトークン\n",
    "new_tokens = terminals_lower - set(converter.tokenizer.get_vocab().keys())\n",
    "print(\"new_tokens:\",new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[942,\n",
       " 291,\n",
       " 293,\n",
       " 294,\n",
       " 306,\n",
       " 320,\n",
       " 331,\n",
       " 334,\n",
       " 337,\n",
       " 341,\n",
       " 333,\n",
       " 359,\n",
       " 350,\n",
       " 491,\n",
       " 506,\n",
       " 423]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = []\n",
    "for i in range(0,16):\n",
    "    token_id = converter.tokenizer.convert_tokens_to_ids(f\"{i}\")\n",
    "    token_ids.append(token_id)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2つ以上のtoken idで表現されている語彙の一覧をリスト形式で出力\n",
    "second_ids = []\n",
    "for terminal in converter.protocol_grammar.terminals:\n",
    "    tokens = converter.tokenizer.encode(terminal)\n",
    "    if len(tokens) > 2:\n",
    "        second_ids.append(tokens[1])\n",
    "        \n",
    "print(second_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.tokenizer.convert_tokens_to_ids(\"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.tokenizer.convert_tokens_to_ids(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.tokenizer.decode([2726,262,320,262],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[262, 9521, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.tokenizer.encode(\"護衛\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[12631], [23501], [23501, 312], [290], [262, 268], [262, 2227]], 'attention_mask': [[1], [1], [1, 1], [1], [1, 1], [1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.tokenizer.batch_encode_plus([\"agent\",\"possess\",\"possessed\",\"(\",\")\",\"03\"], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 既存のTokenをできるだけ流用したConverterのテスト\n",
    "終端記号を小文字にして、既存のTokenをできるだけ流用してみる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, LogitsProcessorList, LogitsProcessor\n",
    "\n",
    "# 日本語プロトコル変換用\n",
    "from aiwolfk2b.agentLPS.jp_to_protocol_converter import JPToProtocolConverter\n",
    "from aiwolfk2b.utils.ll1_grammar import LL1Grammar, aiwolf_protocol_grammar, convert_ll1_to_protocol,convert_protocol_to_ll1\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union,Set\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "\n",
    "\n",
    "#現在のプログラムが置かれているディレクトリを取得\n",
    "import os\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root:TreeNode = TreeNode(token_id=-1,parent=None)\n",
    "    \n",
    "    def add_branch(self, token_ids:List[int],terminal:str = None):\n",
    "        cur_node:TreeNode = self.root\n",
    "        for i in token_ids:\n",
    "            #子ノードがなければ追加\n",
    "            if cur_node.children[i] is None:\n",
    "                cur_node.add_child(TreeNode(token_id=i,parent=cur_node))\n",
    "            #子ノードに移動\n",
    "            cur_node = cur_node.children[i]\n",
    "        #終端記号があれば一番最後に追加\n",
    "        if terminal is not None:\n",
    "            cur_node.terminal = terminal\n",
    "\n",
    "    def print(self, node=None, _prefix=\"\", _last=True):\n",
    "        node = node or self.root\n",
    "        print(_prefix, \"`- \" if _last else \"|- \", node, sep=\"\")\n",
    "        _prefix += \"   \" if _last else \"|  \"\n",
    "        child_count = len(node.children)\n",
    "        for i, child in enumerate(node.children.values()):\n",
    "            _last = i == (child_count - 1)\n",
    "            self.print(child, _prefix, _last)\n",
    "    \n",
    "    def __str__(self):\n",
    "        from io import StringIO\n",
    "        buf = StringIO()\n",
    "        self._print_str(self.root, buf)\n",
    "        return buf.getvalue()\n",
    "    \n",
    "    def _print_str(self, node, buf, _prefix=\"\", _last=True):\n",
    "        buf.write(_prefix)\n",
    "        buf.write(\"`- \" if _last else \"|- \")\n",
    "        buf.write(str(node))\n",
    "        buf.write(\"\\n\")\n",
    "        _prefix += \"   \" if _last else \"|  \"\n",
    "        child_count = len(node.children)\n",
    "        for i, child in enumerate(node.children.values()):\n",
    "            _last = i == (child_count - 1)\n",
    "            self._print_str(child, buf, _prefix, _last)\n",
    "        \n",
    "class TreeNode:\n",
    "    def __init__(self,token_id, parent, terminal=None, is_leaf=True):\n",
    "        self.token_id = token_id\n",
    "        self.terminal = terminal\n",
    "        self.parent = parent\n",
    "        self.is_leaf = is_leaf\n",
    "        self.__children = defaultdict(lambda: None)\n",
    "    \n",
    "    @property\n",
    "    def children(self):\n",
    "        return self.__children\n",
    "    \n",
    "    # 子ノードを追加する\n",
    "    def add_child(self, child):\n",
    "        self.__children[child.token_id] = child\n",
    "        # 子ノードが追加されたので、葉ではなくなる\n",
    "        if self.is_leaf:\n",
    "            self.is_leaf = False\n",
    "            \n",
    "    # subtreeの終端記号のセットを返す\n",
    "    def get_subtree_terminals(self)->Set[str]:\n",
    "        terminals:Set[str] = set()\n",
    "        if self.terminal is not None:\n",
    "            terminals.add(self.terminal)\n",
    "            \n",
    "        for child in self.children.values():\n",
    "            terminals = terminals | child.get_subtree_terminals()\n",
    "        return terminals\n",
    "    \n",
    "    #自分以下の部分木が瞬時符号になっているか確認\n",
    "    def is_instant(self)->Tuple[bool,List[str]]:\n",
    "        if self.is_leaf:\n",
    "            return (True,[])\n",
    "        conflict_pairs = []\n",
    "        is_ok = True\n",
    "        #葉ノード以外に終端記号がある場合は瞬時符号ではない\n",
    "        if self.terminal is not None:\n",
    "            conflict_pairs.append((self.token_id,self.terminal))\n",
    "            is_ok = False\n",
    "            \n",
    "        #子ノードに対して繰り返し適用\n",
    "        for child in self.children.values():\n",
    "            b,pairs = child.is_instant()\n",
    "            is_ok = is_ok and b\n",
    "            conflict_pairs.extend(pairs)\n",
    "        #すベての子ノードが瞬時符号ならば瞬時符号\n",
    "        return (is_ok,conflict_pairs)\n",
    "        \n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Node({self.token_id}, Terminal={self.terminal})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 文法に従ったトークンのみを生成するlogits_processor\n",
    "class ConstrainedLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, grammar: LL1Grammar, tokenizer: T5Tokenizer):\n",
    "        super().__init__()\n",
    "        self.grammar:LL1Grammar = grammar\n",
    "        self.tokenizer:T5Tokenizer = tokenizer\n",
    "        # self.space_token_id = self.tokenizer.convert_tokens_to_ids(\" \")\n",
    "        # print(\"space_token_id:\",self.space_token_id)\n",
    "        self.token_tree = self.generate_token_tree()\n",
    "        \n",
    "    def generate_token_tree(self)->Tree:\n",
    "        #tokenizerとgrammarを用いて、終端記号トークンの木を生成する\n",
    "        tree = Tree()\n",
    "        terminals = list(self.grammar.terminals)\n",
    "        #小文字に変換\n",
    "        for i,terminal in enumerate(terminals):\n",
    "            terminals[i] = terminal.lower()\n",
    "        terminals_tokens_ids = self.tokenizer.batch_encode_plus(terminals,add_special_tokens=False)[\"input_ids\"]\n",
    "        for terminal,terminal_tokens_ids in zip(terminals,terminals_tokens_ids):\n",
    "            tree.add_branch(terminal_tokens_ids,terminal)\n",
    "            \n",
    "        return tree    \n",
    "        \n",
    "    def __call__(self, input_ids, scores):\n",
    "        #print(input_ids)\n",
    "        #一度文章に変換\n",
    "        protocol_batch = self.tokenizer.batch_decode(input_ids,skip_special_tokens=True)\n",
    "        valid_tokens_ids = []\n",
    "        agent_numbers_ids_1_15 = {2072, 1905, 2078, 2458, 2357, 2195, 2227, 2246, 2224,333,359,350,491,506,423}\n",
    "        numbers_ids_0_9 = {942, 291, 293, 294, 306, 320, 331, 334, 337, 341}\n",
    "        #各文章をスペースで分割\n",
    "        for idx,protocol in enumerate(protocol_batch):\n",
    "            possible_tokens_ids = set()\n",
    "            #protocol_split = protocol.split()\n",
    "            #空の場合\n",
    "            print(\"protocol:\",protocol)\n",
    "            print(\"protocol_ids:\",input_ids[idx])\n",
    "            if protocol == \"\":\n",
    "                #最初に来るトークンの候補を取得\n",
    "                possible_terminals = self.grammar.first_sets[self.grammar.start_symbol]\n",
    "                # print(\"possible_terminals:\",possible_terminals)\n",
    "                for terminal in possible_terminals:\n",
    "                    terminal_token_id = self.tokenizer.convert_tokens_to_ids(terminal)\n",
    "                    possible_tokens_ids.add(terminal_token_id)\n",
    "            #最後のトークンが\"day\"の場合、次にありえるトークンを設定\n",
    "            elif input_ids[idx][-1] == 2726: # \"day\" <-> 2726\n",
    "                possible_tokens_ids.add(262)#\" \" <-> 262\n",
    "            #最後のトークンが01~15の場合、次にありえるトークンを設定\n",
    "            elif int(input_ids[idx][-1]) in agent_numbers_ids_1_15:\n",
    "                # print(\"in agent_numbers_ids_1_15\")\n",
    "                possible_tokens_ids.add(262)#\" \" <-> 262\n",
    "            #最後のトークンが0~9の場合、次にありえるトークンを設定\n",
    "            elif int(input_ids[idx][-1]) in numbers_ids_0_9:\n",
    "                # print(\"in numbers_ids_0_9\")\n",
    "                possible_tokens_ids.add(262)#\" \" <-> 262\n",
    "            \n",
    "            # #最後のトークンが262の場合、次にありえるトークンを設定\n",
    "            # elif input_ids[idx][-1] == 262:\n",
    "            #     #01~09, )が該当\n",
    "            #     after262_tokens = [1905, 2078, 2224, 2357, 2458, 2246, 2072, 2227, 2195, 268]\n",
    "            #     possible_terminals = self.grammar.get_next_terminals(protocol)\n",
    "            #     possible_terminals_tokens = set()\n",
    "            #     for terminal in possible_terminals:\n",
    "            #         possible_terminals_tokens.add(self.tokenizer.convert_tokens_to_ids(terminal))\n",
    "            #     possible_tokens_ids.add(after262_tokens & possible_terminals)\n",
    "            #最後のトークンが262以外の場合は、次に来る終端記号がそのまま続く\n",
    "            \n",
    "            #次に来る終端記号がそのまま続く\n",
    "            else:    \n",
    "                possible_terminals = self.grammar.get_next_terminals(protocol)\n",
    "                # print(\"possible_terminals:\",possible_terminals)\n",
    "                for terminal in possible_terminals:\n",
    "                    terminal_token_id = self.tokenizer.convert_tokens_to_ids(terminal)\n",
    "                    if terminal == \"(\":\n",
    "                        possible_tokens_ids.add(290)\n",
    "                    elif terminal == \")\":\n",
    "                        if input_ids[idx][-1] == 262: #すでに　空白(262)がある場合\n",
    "                            possible_tokens_ids.add(268)\n",
    "                        else:\n",
    "                            possible_tokens_ids.add(262) #\"number )\"を実現するために、262を追加\n",
    "                    elif terminal == \"ε\":\n",
    "                        possible_tokens_ids.add(self.tokenizer.eos_token_id)\n",
    "                    else:\n",
    "                        possible_tokens_ids.add(terminal_token_id)\n",
    "                    \n",
    "                    \n",
    "                            \n",
    "            #もし、得られる終端記号の候補がない場合は、終了記号を追加\n",
    "            if len(possible_tokens_ids) == 0:\n",
    "                possible_tokens_ids.add(self.tokenizer.eos_token_id)\n",
    "                \n",
    "            valid_tokens_ids.append(possible_tokens_ids)\n",
    "\n",
    "        #print(\"eos_token_id:{}\".format(self.tokenizer.eos_token_id))\n",
    "        print(\"valid_token_ids:{}\".format(valid_tokens_ids))\n",
    "        # print(\"scores.shape:{}\".format(scores.shape))\n",
    "\n",
    "        for batch_idx in range(scores.shape[0]):\n",
    "            for token_id in range(scores.shape[1]):\n",
    "                if token_id not in valid_tokens_ids[batch_idx]:\n",
    "                    scores[batch_idx, token_id] = float('-inf')\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5JPToProtocolConverter(JPToProtocolConverter):\n",
    "    def __init__(self, model_name: str = \"default\", model_path: str = \"default\"):\n",
    "        #モデルの読み込み\n",
    "        if model_name == \"default\":\n",
    "            MODEL_NAME = \"sonoisa/t5-base-english-japanese\"\n",
    "        else:\n",
    "            MODEL_NAME = model_name\n",
    "        if model_path == \"default\":\n",
    "            MODEL_PATH = \"/home/takuya/HDD1/work/AI_Wolf/2023S_AIWolfK2B/aiwolfk2b/agentLPS/jp2protocol_model/t5_upper_20230514_4.pth\"\n",
    "        else:\n",
    "            MODEL_PATH = model_path\n",
    "            \n",
    "        self.model_name = MODEL_NAME\n",
    "        self.model_path = MODEL_PATH\n",
    "        \n",
    "        self.model:T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "        self.tokenizer:T5Tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        # プロトコル変換用に文法の準備\n",
    "        self.protocol_grammar: LL1Grammar = aiwolf_protocol_grammar()\n",
    "        # プロトコル変換用のlogits_processorの準備\n",
    "        self.logits_processor = LogitsProcessorList([\n",
    "            ConstrainedLogitsProcessor(self.protocol_grammar, self.tokenizer)\n",
    "        ])\n",
    "        #トークナイザーにLL1文法の終端記号を追加し、モデルのトークナイザーをリサイズ\n",
    "        # new_tokens = self.protocol_grammar.terminals - set(self.tokenizer.get_vocab().keys())\n",
    "        # self.tokenizer.add_tokens(list(new_tokens))\n",
    "        # self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        #学習したモデルがあれば読み込む\n",
    "        if MODEL_PATH != \"\":\n",
    "            self.model.load_state_dict(torch.load(MODEL_PATH,map_location=torch.device(device)))\n",
    "        \n",
    "    def convert(self, text_list: List[str]) -> List[str]:\n",
    "        input = self.tokenizer.batch_encode_plus(text_list, max_length=512, padding=True, return_tensors='pt', truncation=True)\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "        inputs=input[\"input_ids\"],\n",
    "        attention_mask=input[\"attention_mask\"],\n",
    "        #force_words_ids=self.force_words_ids,\n",
    "        num_beams=5,\n",
    "        temperature=1,\n",
    "        #do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=0,\n",
    "        remove_invalid_values=False,\n",
    "        logits_processor=self.logits_processor,\n",
    "        max_length = 16,\n",
    "        early_stopping=True,\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unit_test_T5JPToProtocolConverter():\n",
    "    converter = T5JPToProtocolConverter()\n",
    "    text = \"Agent[08]が襲われたAgent[05]を霊媒すると人間だった\"\n",
    "    # 入力する文章\n",
    "    text_list = [\n",
    "        \"Agent[03]はAgent[08]が狼だと推測する\",\n",
    "        # \"Agent[06]はAgent[06]が占い師だとカミングアウトする\",\n",
    "        # \"Agent[12]が占った結果Agent[10]は人狼だった\",\n",
    "        # \"Agent[12]が占った結果Agent[10]は人間だった\",\n",
    "        # \"Agent[08]が襲われたAgent[05]を霊媒すると人間だった\",\n",
    "        # \"Agent[05]はAgent[10]を護衛した\",\n",
    "        # \"Agent[10]はAgent[12]に投票する\",\n",
    "        # \"Agent[06]はAgent[08]が狼だと思う\",\n",
    "        # \"私が占い師です\",\n",
    "        # \"Agent[12]が占った結果、Agent[10]は人狼でした\",\n",
    "        # \"Agent[12]が占った結果、Agent[10]は人間でした\",\n",
    "        # \"Agent[12]がAgent[05]を霊媒すると人間でした\",\n",
    "        # \"Agent[12]はAgent[10]を守った\",\n",
    "        # \"Agent[10]はAgent[12]に投票します\",\n",
    "        # \"Agent[08]が狼だと思う\",\n",
    "        # \"私が占い師です\",\n",
    "        # \"占った結果、Agent[10]は人狼でした\",\n",
    "        # \"占った結果、Agent[10]は人間でした\",\n",
    "        # \"Agent[05]を霊媒すると人間でした\",\n",
    "        # \"私はAgent[10]を守った\",\n",
    "        # \"私はAgent[12]に投票します\",\n",
    "    ]\n",
    "    protocol = converter.convert([text])\n",
    "    print(\"one text:\",protocol)\n",
    "    \n",
    "    protocols = converter.convert(text_list)\n",
    "    print(\"text_list:\", protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = T5JPToProtocolConverter(model_path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`- Node(-1, Terminal=None)\n",
      "   |- Node(2674, Terminal=0)\n",
      "   |- Node(637, Terminal=over)\n",
      "   |- Node(386, Terminal=or)\n",
      "   |- Node(3724, Terminal=None)\n",
      "   |  `- Node(18339, Terminal=bodyguard)\n",
      "   |- Node(451, Terminal=not)\n",
      "   |- Node(7218, Terminal=guard)\n",
      "   |  `- Node(312, Terminal=guarded)\n",
      "   |- Node(685, Terminal=5)\n",
      "   |- Node(262, Terminal=None)\n",
      "   |  |- Node(2227, Terminal=03)\n",
      "   |  |- Node(29475, Terminal=estimate)\n",
      "   |  |- Node(2357, Terminal=07)\n",
      "   |  |- Node(2246, Terminal=02)\n",
      "   |  |- Node(2072, Terminal=05)\n",
      "   |  |- Node(268, Terminal=))\n",
      "   |  |- Node(2224, Terminal=06)\n",
      "   |  |- Node(2195, Terminal=09)\n",
      "   |  |- Node(1905, Terminal=01)\n",
      "   |  |- Node(2078, Terminal=08)\n",
      "   |  `- Node(2458, Terminal=04)\n",
      "   |- Node(834, Terminal=7)\n",
      "   |- Node(7528, Terminal=None)\n",
      "   |  `- Node(408, Terminal=skip)\n",
      "   |- Node(1725, Terminal=None)\n",
      "   |  `- Node(370, Terminal=villager)\n",
      "   |- Node(4830, Terminal=attack)\n",
      "   |- Node(861, Terminal=8)\n",
      "   |- Node(1233, Terminal=13)\n",
      "   |- Node(605, Terminal=4)\n",
      "   |- Node(13831, Terminal=id)\n",
      "   |- Node(280, Terminal=and)\n",
      "   |- Node(924, Terminal=9)\n",
      "   |- Node(20840, Terminal=voted)\n",
      "   |- Node(504, Terminal=2)\n",
      "   |- Node(575, Terminal=3)\n",
      "   |- Node(290, Terminal=()\n",
      "   |- Node(9322, Terminal=request)\n",
      "   |- Node(500, Terminal=1)\n",
      "   |- Node(1210, Terminal=14)\n",
      "   |- Node(1047, Terminal=None)\n",
      "   |  `- Node(614, Terminal=xor)\n",
      "   |- Node(18812, Terminal=attacked)\n",
      "   |- Node(8129, Terminal=vote)\n",
      "   |- Node(279, Terminal=None)\n",
      "   |  `- Node(3144, Terminal=None)\n",
      "   |     `- Node(530, Terminal=inquire)\n",
      "   |- Node(400, Terminal=None)\n",
      "   |  `- Node(29949, Terminal=werewolf)\n",
      "   |- Node(1651, Terminal=because)\n",
      "   |- Node(1048, Terminal=None)\n",
      "   |  `- Node(3971, Terminal=None)\n",
      "   |     `- Node(799, Terminal=divination)\n",
      "   |- Node(1458, Terminal=None)\n",
      "   |  `- Node(358, Terminal=seer)\n",
      "   |- Node(828, Terminal=6)\n",
      "   |- Node(886, Terminal=12)\n",
      "   |- Node(2199, Terminal=human)\n",
      "   |- Node(23501, Terminal=None)\n",
      "   |  `- Node(312, Terminal=possessed)\n",
      "   |- Node(8602, Terminal=None)\n",
      "   |  `- Node(2845, Terminal=comingout)\n",
      "   |- Node(20427, Terminal=ε)\n",
      "   |- Node(12631, Terminal=agent)\n",
      "   |- Node(1307, Terminal=any)\n",
      "   |- Node(30835, Terminal=disagree)\n",
      "   |- Node(966, Terminal=11)\n",
      "   |- Node(28316, Terminal=None)\n",
      "   |  `- Node(321, Terminal=divined)\n",
      "   |- Node(1219, Terminal=day)\n",
      "   |- Node(16422, Terminal=agree)\n",
      "   |- Node(955, Terminal=15)\n",
      "   |- Node(13566, Terminal=identified)\n",
      "   |- Node(746, Terminal=10)\n",
      "   `- Node(17535, Terminal=medium)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(converter.logits_processor[0].token_tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token列から終端記号列の復元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得られたtoken_treeがすべての終端記号を含んでいるか確認(=ちゃんとすべての終端記号を木が表現できているか確認)\n",
    "token_tree_terminals = converter.logits_processor[0].token_tree.root.get_subtree_terminals()\n",
    "terminals = list(converter.protocol_grammar.terminals)\n",
    "#小文字に変換\n",
    "for i,terminal in enumerate(terminals):\n",
    "    terminals[i] = terminal.lower()\n",
    "terminals = set(terminals)\n",
    "\n",
    "token_tree_terminals >= terminals # Trueなので問題なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'agent 12 divined agent 10 human'\n",
    "token_ids = converter.tokenizer.encode_plus(text,add_special_tokens=False)[\"input_ids\"]\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12631, 886, 7218, 312, 12631, 746, 2199]\n",
      "['▁agent', '▁12', '▁guard', 'ed', '▁agent', '▁10', '▁human']\n",
      "[12631, 886, 7218, 312, 12631, 746, 2199]\n"
     ]
    }
   ],
   "source": [
    "text = 'agent 12 guarded agent 10 human'\n",
    "token_ids = converter.tokenizer.encode_plus(text,add_special_tokens=False)[\"input_ids\"]\n",
    "print(token_ids)\n",
    "tokens = converter.tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "print(converter.tokenizer.convert_tokens_to_ids(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, [(7218, 'guard')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#瞬時符号か確認\n",
    "token_tree:Tree =converter.logits_processor[0].token_tree\n",
    "token_tree.root.is_instant() # Falseで瞬時符号ではない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#木の深さまでみれば復号はできる\n",
    "token_tree:Tree =converter.logits_processor[0].token_tree\n",
    "cur_node = token_tree.root\n",
    "terminals_list = []\n",
    "for token_id in token_ids:\n",
    "    if cur_node.is_leaf:\n",
    "        terminals_list.append(cur_node.terminal)\n",
    "        cur_node = token_tree.root\n",
    "    else:\n",
    "        \n",
    "        cur_node = cur_node.children[token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[262, 10170, 84, 82, 78, 83, 76, 84, 90, 89], [262, 93, 84, 87]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.tokenizer.batch_encode_plus([\"COMINGOUT\",\"XOR\"],add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01', 'seer', '08', 'guarded', '14', '10', 'because', 'estimate', '0', 'comingout', '09', 'vote', '6', 'agree', 'bodyguard', 'attack', '07', '13', 'skip', '7', '03', 'id', 'possessed', '1', 'divined', 'day', 'guard', 'agent', '11', 'identified', 'inquire', 'divination', 'and', 'request', '3', 'werewolf', ')', 'voted', 'medium', 'over', '8', '04', '4', '(', 'villager', '15', '12', 'any', '05', 'disagree', '5', 'human', 'or', '2', 'not', 'attacked', '06', 'xor', 'ε', '9', '02'}\n",
      "{'guard', 'guarded'}\n",
      "{'01', '08', '07', '05', 'estimate', '03', ')', '09', '06', '04', '02'}\n"
     ]
    }
   ],
   "source": [
    "print(converter.logits_processor[0].token_tree.root.get_subtree_terminals())\n",
    "print(converter.logits_processor[0].token_tree.root.children[7218].get_subtree_terminals())\n",
    "print(converter.logits_processor[0].token_tree.root.children[262].get_subtree_terminals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guard', 'guarded'}\n"
     ]
    }
   ],
   "source": [
    "print(converter.logits_processor[0].token_tree.root.children[7218].get_subtree_terminals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agent[12] divined agent[10] human'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\"Agent[12] DIVINED Agent[10] HUMAN\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得られたtoken_treeがすべての終端記号を含んでいるか確認\n",
    "token_tree_terminals = converter.logits_processor[0].token_tree.root.get_subtree_terminals()\n",
    "terminals = list(converter.protocol_grammar.terminals)\n",
    "#小文字に変換\n",
    "for i,terminal in enumerate(terminals):\n",
    "    terminals[i] = terminal.lower()\n",
    "terminals = set(terminals)\n",
    "\n",
    "token_tree_terminals >= terminals # Trueなので問題なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_k2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8993d439cf46e30da0252341e241d2eb3d9fc7857352dee07a233f130e5e9d15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
