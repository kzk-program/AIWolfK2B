{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTのクラス分類で役職を推定するモデルを構成し、学習させる"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiwolf import Role,Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 128\n",
    "# 文章をトークンに変換するトークナイザーの読み込み\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification_pl(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        # model_name: Transformersのモデルの名前\n",
    "        # num_labels: ラベルの数\n",
    "        # lr: 学習率\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # 引数のnum_labelsとlrを保存。\n",
    "        # 例えば、self.hparams.lrでlrにアクセスできる。\n",
    "        # チェックポイント作成時にも自動で保存される。\n",
    "        self.save_hyperparameters() \n",
    "\n",
    "        # BERTのロード\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        \n",
    "    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数を書く。\n",
    "    # batch_idxはミニバッチの番号であるが今回は使わない。\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss) # 損失を'train_loss'の名前でログをとる。\n",
    "        return loss\n",
    "        \n",
    "    # 検証データのミニバッチが与えられた時に、\n",
    "    # 検証データを評価する指標を計算する関数を書く。\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss) # 損失を'val_loss'の名前でログをとる。\n",
    "\n",
    "    # テストデータのミニバッチが与えられた時に、\n",
    "    # テストデータを評価する指標を計算する関数を書く。\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels') # バッチからラベルを取得\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = ( labels_predicted == labels ).sum().item()\n",
    "        accuracy = num_correct/labels.size(0) #精度\n",
    "        self.log('accuracy', accuracy) # 精度を'accuracy'の名前でログをとる。\n",
    "\n",
    "    # 学習に用いるオプティマイザを返す関数を書く。\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_loader = []\n",
    "\n",
    "# データを取得\n",
    "#現在のディレクトリを取得\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = pathlib.Path().resolve()\n",
    "data_set_plain = pickle.load(open(current_dir.joinpath(\"preprocessed_data\").joinpath('dataset.pkl'), 'rb'))\n",
    "labels_list = [Role.VILLAGER,Role.SEER,Role.BODYGUARD,Role.MEDIUM,Role.WEREWOLF,Role.POSSESSED,Role.FOX,Role.FREEMASON]\n",
    "\n",
    "for data in data_set_plain:\n",
    "    encoding = tokenizer(\n",
    "            data[1],\n",
    "            max_length=MAX_LENGTH, \n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    encoding['labels'] = labels_list.index(Role(data[0]))\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader) # ランダムにシャッフル\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n",
    "\n",
    "# データセットからデータローダを作成\n",
    "# 学習データはshuffle=Trueにする。\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE//4, shuffle=True\n",
    ") \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/robot_dev6/okubo/aiwolf/AIWolfK2B/k2b_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "# 学習時にモデルの重みを保存する条件を指定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    save_weights_only=True,\n",
    "    dirpath='bert_role_estimation_model/',\n",
    ")\n",
    "\n",
    "# 学習の方法を指定\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "\n",
      "  | Name    | Type                          | Params\n",
      "----------------------------------------------------------\n",
      "0 | bert_sc | BertForSequenceClassification | 110 M \n",
      "----------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.494   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b86e95ec0a645698d57422254c70398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robot_dev6/okubo/aiwolf/AIWolfK2B/k2b_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/robot_dev6/okubo/aiwolf/AIWolfK2B/k2b_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4773c9f797c74e6485d68b5e8365a4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca49dced1684f86bca8f6c7f3329a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461378a98628424189e3871fb4dc7546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e262b6b217c044cb8c569fb15ea9528f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aefb1fb5a614623af1efa7389cafd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65010aee0c04488bac3e0ad3b416a2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27951b6c66074d899124755202dda20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d8e8bded4641a48648019bcfa47d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bd8d48008b4c44940ab6d056b76c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d8c197cc3646abb4fd369728b0b78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0dd618f04a4d6ca504bdbe5b5c9d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Lightningモデルのロード\n",
    "model = BertForSequenceClassification_pl(\n",
    "    MODEL_NAME, num_labels=len(labels_list), lr=1e-5\n",
    ")\n",
    "\n",
    "# ファインチューニングを行う。\n",
    "trainer.fit(model, dataloader_train, dataloader_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ベストモデルのファイル:  /storage/home/robot_dev6/okubo/aiwolf/AIWolfK2B/aiwolfk2b/AttentionReasoningAgent/Modules/bert_role_estimation_model/epoch=9-step=11530.ckpt\n",
      "ベストモデルの検証データに対する損失:  tensor(0.3224, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 検証データで確認\n",
    "best_model_path = checkpoint.best_model_path # ベストモデルのファイル\n",
    "print('ベストモデルのファイル: ', checkpoint.best_model_path)\n",
    "print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robot_dev6/okubo/aiwolf/AIWolfK2B/k2b_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:148: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /storage/home/robot_dev6/okubo/aiwolf/AIWolfK2B/aiwolfk2b/AttentionReasoningAgent/Modules/bert_role_estimation_model/epoch=9-step=11530.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "Loaded model weights from the checkpoint at /storage/home/robot_dev6/okubo/aiwolf/AIWolfK2B/aiwolfk2b/AttentionReasoningAgent/Modules/bert_role_estimation_model/epoch=9-step=11530.ckpt\n",
      "/home/robot_dev6/okubo/aiwolf/AIWolfK2B/k2b_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6024f1c54c4180a5280f69ea12272b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        accuracy            0.8543768525123596\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# テストデータで確認\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers対応のモデルを保存\n",
    "out_model_dir = './bert_role_estimation_model'\n",
    "\n",
    "import datetime\n",
    "today = datetime.date.today()\n",
    "out_filename = format(today, '%Y%m%d')\n",
    "\n",
    "torch.save(model.bert_sc.state_dict(),out_model_dir + '/' + f'{out_filename}.pth')\n",
    "model.bert_sc.save_pretrained('./bert_role_estimation_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルのロード\n",
    "best_model_path = \"./bert_role_estimation_model/20230625.pth\"\n",
    "bert_sc_best = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=len(labels_list),\n",
    "        )\n",
    "bert_sc_best.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存したモデルが取得できるか確認\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    './bert_role_estimation_model'\n",
    ")\n",
    "bert_sc = bert_sc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Role.WEREWOLF: 'WEREWOLF'>, <Role.VILLAGER: 'VILLAGER'>]\n"
     ]
    }
   ],
   "source": [
    "#学習したモデルの検証\n",
    "test_input = [\"\"\"4,1,0,0,1,1,1,0\n",
    "SEER\n",
    "day1\n",
    "talk:\n",
    "もはもは＾－＾\n",
    "もはもは＾－＾\n",
    "もは＾－＾占いCO[02]たん◯れした＾－＾\n",
    "もは＾－＾\n",
    "もは＾－＾寒い所では…花は枯れてしまうの…；－；\n",
    "もはよう＾－＾\n",
    "占い把握＾－＾\n",
    "[03]たん占い把握＾－＾\n",
    "占いco[08]○\n",
    "占い把握＾－＾\n",
    "占い2把握＾－＾\n",
    "[04]たんも占いね＾－＾\n",
    "占い2把握ら＾－＾真狂か真狐とかかのあ＾－＾\n",
    "人外全潜伏かしら＾－＾狂人はいなさそう？＾－＾\n",
    "対抗把握しマス＾－＾\n",
    "役職欠けて狂狐の可能性も＾－＾\n",
    ">>13狂いない可能性もある＾－＾\n",
    "対抗把握＾－＾狂>狼狐かな＾－＾\n",
    "占い2出てるから全潜伏れはないれそ＾－＾真狂めかのあ＾－＾\n",
    "[04]たんちょっと出方様子見っぽく思えたから真目下がるのあ；－；\n",
    "狂人いなくて占い欠けの狼狐らったらやばえ；－；\n",
    ">>17おんその場合は狼か狐が出てるのもあるなって＾－＾\n",
    "とりま占い先宣言してほしいお＾－＾\n",
    "今日はグレラン？＾－＾\n",
    "グレーから柱出てもらう？＾－＾吊りあんもしゆゆうのいけお＾－＾\n",
    "呪殺ないと真なのかまからん；－；\n",
    "吊りは狐先に吊らないとら＾－＾\n",
    "じゃあ[07]たん占う＾－＾\n",
    "宣言したほうがいいかんじ？＾－＾\n",
    "吊り余裕は銃殺出してもらえば増えるし対抗占いしてもらいたいかも＾－＾\n",
    "じゃあ[06]たん行きます\n",
    "漏れ吊っていいお＾－＾\n",
    "day2\n",
    "divine,1,4,HUMAN\n",
    "talk:\"\"\",\"\"\"4,1,0,0,1,1,1,0\n",
    "VILLAGER\n",
    "day1\n",
    "talk:\n",
    "もはもは＾－＾\n",
    "もはもは＾－＾\n",
    "もは＾－＾占いCO[01]たん◯れした＾－＾\n",
    "もは＾－＾\n",
    "もは＾－＾寒い所では…花は枯れてしまうの…；－；\n",
    "もはよう＾－＾\n",
    "占い把握＾－＾\n",
    "[02]たん占い把握＾－＾\n",
    "占いco[07]○\n",
    "占い把握＾－＾\n",
    "占い2把握＾－＾\n",
    "[03]たんも占いね＾－＾\n",
    "占い2把握ら＾－＾真狂か真狐とかかのあ＾－＾\n",
    "人外全潜伏かしら＾－＾狂人はいなさそう？＾－＾\n",
    "対抗把握しマス＾－＾\n",
    "役職欠けて狂狐の可能性も＾－＾\n",
    ">>13狂いない可能性もある＾－＾\n",
    "対抗把握＾－＾狂>狼狐かな＾－＾\n",
    "占い2出てるから全潜伏れはないれそ＾－＾真狂めかのあ＾－＾\n",
    "[03]たんちょっと出方様子見っぽく思えたから真目下がるのあ；－；\n",
    "狂人いなくて占い欠けの狼狐らったらやばえ；－；\n",
    ">>17おんその場合は狼か狐が出てるのもあるなって＾－＾\n",
    "とりま占い先宣言してほしいお＾－＾\n",
    "今日はグレラン？＾－＾\n",
    "グレーから柱出てもらう？＾－＾吊りあんもしゆゆうのいけお＾－＾\n",
    "呪殺ないと真なのかまからん；－；\n",
    "吊りは狐先に吊らないとら＾－＾\n",
    "じゃあ[06]たん占う＾－＾\n",
    "宣言したほうがいいかんじ？＾－＾\n",
    "吊り余裕は銃殺出してもらえば増えるし対抗占いしてもらいたいかも＾－＾\n",
    "じゃあ[05]たん行きます\n",
    "漏れ吊っていいお＾－＾\n",
    "\"\"\"]\n",
    "\n",
    "truth_label = [Role.WEREWOLF, Role.VILLAGER]\n",
    "\n",
    "encoding = tokenizer(\n",
    "        test_input,\n",
    "        max_length=MAX_LENGTH, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "# 推論\n",
    "with torch.no_grad():\n",
    "    output = bert_sc.forward(**encoding)\n",
    "scores = output.logits # 分類スコア\n",
    "labels_predicted = scores.argmax(-1).cpu().numpy() # スコアが最も高いラベル\n",
    "labels_predicted = [labels_list[i] for i in labels_predicted] # ラベルをラベル名に変換\n",
    "print(labels_predicted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_k2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
