{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTのクラス分類で役職を推定するモデルを構成し、学習させる"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiwolf import Role"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "MAX_LENGTH = 512\n",
    "# 文章をトークンに変換するトークナイザーの読み込み\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification_pl(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        # model_name: Transformersのモデルの名前\n",
    "        # num_labels: ラベルの数\n",
    "        # lr: 学習率\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # 引数のnum_labelsとlrを保存。\n",
    "        # 例えば、self.hparams.lrでlrにアクセスできる。\n",
    "        # チェックポイント作成時にも自動で保存される。\n",
    "        self.save_hyperparameters() \n",
    "\n",
    "        # BERTのロード\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        \n",
    "    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数を書く。\n",
    "    # batch_idxはミニバッチの番号であるが今回は使わない。\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss) # 損失を'train_loss'の名前でログをとる。\n",
    "        return loss\n",
    "        \n",
    "    # 検証データのミニバッチが与えられた時に、\n",
    "    # 検証データを評価する指標を計算する関数を書く。\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss) # 損失を'val_loss'の名前でログをとる。\n",
    "\n",
    "    # テストデータのミニバッチが与えられた時に、\n",
    "    # テストデータを評価する指標を計算する関数を書く。\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels') # バッチからラベルを取得\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = ( labels_predicted == labels ).sum().item()\n",
    "        accuracy = num_correct/labels.size(0) #精度\n",
    "        self.log('accuracy', accuracy) # 精度を'accuracy'の名前でログをとる。\n",
    "\n",
    "    # 学習に用いるオプティマイザを返す関数を書く。\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習時にモデルの重みを保存する条件を指定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='bert_role_estimation_model/',\n",
    ")\n",
    "\n",
    "# 学習の方法を指定\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=10,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightningモデルのロード\n",
    "model = BertForSequenceClassification_pl(\n",
    "    MODEL_NAME, num_labels=len(Role), lr=1e-5\n",
    ")\n",
    "\n",
    "# ファインチューニングを行う。\n",
    "trainer.fit(model, dataloader_train, dataloader_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証データで確認\n",
    "best_model_path = checkpoint.best_model_path # ベストモデルのファイル\n",
    "print('ベストモデルのファイル: ', checkpoint.best_model_path)\n",
    "print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータで確認\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers対応のモデルを保存\n",
    "out_model_dir = './bert_role_estimation_model'\n",
    "\n",
    "import datetime\n",
    "today = datetime.date.today()\n",
    "out_filename = format(today, '%Y%m%d')\n",
    "\n",
    "torch.save(model.bert_sc.state_dict(),out_model_dir + '/' + f'{out_filename}.pth')\n",
    "model.bert_sc.save_pretrained('./bert_role_estimation_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのロード\n",
    "best_model_path = \"./jp2prompt_model/20221225.pth\"\n",
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存したモデルが取得できるか確認\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    './bert_role_estimation_model'\n",
    ")\n",
    "bert_sc = bert_sc.cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_k2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
