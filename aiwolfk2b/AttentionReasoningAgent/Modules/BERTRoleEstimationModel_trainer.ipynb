{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTのクラス分類で役職を推定するモデルを構成し、学習させる"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiwolf import Role,Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 128\n",
    "# 文章をトークンに変換するトークナイザーの読み込み\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification_pl(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels,label_names, lr):\n",
    "        # model_name: Transformersのモデルの名前\n",
    "        # num_labels: ラベルの数\n",
    "        # lr: 学習率\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # 引数のnum_labelsとlrを保存。\n",
    "        # 例えば、self.hparams.lrでlrにアクセスできる。\n",
    "        # チェックポイント作成時にも自動で保存される。\n",
    "        self.save_hyperparameters()\n",
    "        self.label_names = label_names\n",
    "\n",
    "        # BERTのロード\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.weight = torch.tensor([0.3, 1.0, 1.0, 1.0, 1.0,1.0,1.0,1.0]).cuda()\n",
    "        \n",
    "    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数を書く。\n",
    "    # batch_idxはミニバッチの番号であるが今回は使わない。\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        #loss = output.loss\n",
    "        logits = output['logits']\n",
    "        #villagerの重みを下げる\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=self.weight)\n",
    "        loss = criterion(logits, batch['labels'])\n",
    "                \n",
    "        self.log('train_loss', loss) # 損失を'train_loss'の名前でログをとる。\n",
    "        return loss\n",
    "        \n",
    "    # 検証データのミニバッチが与えられた時に、\n",
    "    # 検証データを評価する指標を計算する関数を書く。\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        # val_loss = output.loss\n",
    "        #loss = output.loss\n",
    "        logits = output['logits']\n",
    "        #villagerの重みを下げる\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=self.weight)\n",
    "        loss = criterion(logits, batch['labels'])\n",
    "        self.log('val_loss', loss) # 損失を'val_loss'の名前でログをとる。\n",
    "\n",
    "    # テストデータのミニバッチが与えられた時に、\n",
    "    # テストデータを評価する指標を計算する関数を書く。\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels') # バッチからラベルを取得\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = ( labels_predicted == labels ).sum().item()\n",
    "        accuracy = num_correct/labels.size(0) #精度\n",
    "        self.log('micro accuracy', accuracy) # 精度を'accuracy'の名前でログをとる。\n",
    "        #各ラベルの精度を計算\n",
    "        for i,label in enumerate(self.label_names):\n",
    "            num_correct = ( labels_predicted[labels==i] == i ).sum().item()\n",
    "            if labels[labels==i].size(0) == 0:\n",
    "                each_accuracy = -1\n",
    "            else:\n",
    "                each_accuracy = num_correct/labels[labels==i].size(0)\n",
    "            self.log(f\"{label.name}_accuracy\", each_accuracy) # 各ラベル名_accuracyの名前でログをとる。\n",
    "            \n",
    "\n",
    "    # 学習に用いるオプティマイザを返す関数を書く。\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_loader = []\n",
    "\n",
    "# データを取得\n",
    "#現在のディレクトリを取得\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "from aiwolfk2b.AttentionReasoningAgent.Modules.RoleEstimationModelPreprocessor import RoleEstimationModelPreprocessor\n",
    "from aiwolfk2b.utils.helper import load_default_config\n",
    "config = load_default_config()\n",
    "preprocessor:RoleEstimationModelPreprocessor = RoleEstimationModelPreprocessor(config)\n",
    "labels_list = preprocessor.role_label_list\n",
    "\n",
    "current_dir = pathlib.Path().resolve()\n",
    "#data_set_path=current_dir.joinpath(\"data\",\"preprocessed_data\").joinpath('dataset.pkl')\n",
    "data_set_path=current_dir.joinpath(\"data\",\"train\",'dataset_small.pkl')\n",
    "\n",
    "data_set_plain = pickle.load(open(data_set_path, 'rb'))\n",
    "for data in data_set_plain:\n",
    "    encoding = tokenizer(\n",
    "            data[1],\n",
    "            max_length=MAX_LENGTH, \n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        encoding['labels'] = labels_list.index(Role(data[0]))\n",
    "    except:\n",
    "        print(data[0],data[1])\n",
    "        \n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader) # ランダムにシャッフル\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n",
    "\n",
    "# データセットからデータローダを作成\n",
    "# 学習データはshuffle=Trueにする。\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE//4, shuffle=True\n",
    ") \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習時にモデルの重みを保存する条件を指定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    save_weights_only=True,\n",
    "    dirpath='bert_role_estimation_model/',\n",
    ")\n",
    "\n",
    "# 学習の方法を指定\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightningモデルのロード\n",
    "model = BertForSequenceClassification_pl(\n",
    "    MODEL_NAME, num_labels=len(labels_list),label_names=labels_list, lr=1e-5\n",
    ")\n",
    "\n",
    "# ファインチューニングを行う。\n",
    "trainer.fit(model, dataloader_train, dataloader_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証データで確認\n",
    "best_model_path = checkpoint.best_model_path # ベストモデルのファイル\n",
    "print('ベストモデルのファイル: ', checkpoint.best_model_path)\n",
    "print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータで確認\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"micro accuracy\"]:.2f}')\n",
    "for i,label in enumerate(labels_list):\n",
    "    print(f\"{label.name}_accuracy: {test[0][f'{label.name}_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers対応のモデルを保存\n",
    "out_model_dir = './bert_role_estimation_model'\n",
    "\n",
    "import datetime\n",
    "today = datetime.date.today()\n",
    "out_filename = format(today, '%Y%m%d')\n",
    "\n",
    "torch.save(model.bert_sc.state_dict(),out_model_dir + '/' + f'{out_filename}.pth')\n",
    "model.bert_sc.save_pretrained('./bert_role_estimation_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのロード\n",
    "best_model_path = \"./bert_role_estimation_model/20230629.pth\"\n",
    "bert_sc_best = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=len(labels_list),\n",
    "        )\n",
    "bert_sc_best.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存したモデルが取得できるか確認\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    './bert_role_estimation_model'\n",
    ")\n",
    "bert_sc = bert_sc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習したモデルの検証\n",
    "test_input = [\"\"\"4,1,0,0,1,1,1,0\n",
    "SEER\n",
    "day1\n",
    "talk:\n",
    "もはもは＾－＾\n",
    "もはもは＾－＾\n",
    "もは＾－＾占いCO[02]たん◯れした＾－＾\n",
    "もは＾－＾\n",
    "もは＾－＾寒い所では…花は枯れてしまうの…；－；\n",
    "もはよう＾－＾\n",
    "占い把握＾－＾\n",
    "[03]たん占い把握＾－＾\n",
    "占いco[08]○\n",
    "占い把握＾－＾\n",
    "占い2把握＾－＾\n",
    "[04]たんも占いね＾－＾\n",
    "占い2把握ら＾－＾真狂か真狐とかかのあ＾－＾\n",
    "人外全潜伏かしら＾－＾狂人はいなさそう？＾－＾\n",
    "対抗把握しマス＾－＾\n",
    "役職欠けて狂狐の可能性も＾－＾\n",
    ">>13狂いない可能性もある＾－＾\n",
    "対抗把握＾－＾狂>狼狐かな＾－＾\n",
    "占い2出てるから全潜伏れはないれそ＾－＾真狂めかのあ＾－＾\n",
    "[04]たんちょっと出方様子見っぽく思えたから真目下がるのあ；－；\n",
    "狂人いなくて占い欠けの狼狐らったらやばえ；－；\n",
    ">>17おんその場合は狼か狐が出てるのもあるなって＾－＾\n",
    "とりま占い先宣言してほしいお＾－＾\n",
    "今日はグレラン？＾－＾\n",
    "グレーから柱出てもらう？＾－＾吊りあんもしゆゆうのいけお＾－＾\n",
    "呪殺ないと真なのかまからん；－；\n",
    "吊りは狐先に吊らないとら＾－＾\n",
    "じゃあ[07]たん占う＾－＾\n",
    "宣言したほうがいいかんじ？＾－＾\n",
    "吊り余裕は銃殺出してもらえば増えるし対抗占いしてもらいたいかも＾－＾\n",
    "じゃあ[06]たん行きます\n",
    "漏れ吊っていいお＾－＾\n",
    "day2\n",
    "divine,1,4,HUMAN\n",
    "talk:\"\"\",\"\"\"4,1,0,0,1,1,1,0\n",
    "VILLAGER\n",
    "day1\n",
    "talk:\n",
    "もはもは＾－＾\n",
    "もはもは＾－＾\n",
    "もは＾－＾占いCO[01]たん◯れした＾－＾\n",
    "もは＾－＾\n",
    "もは＾－＾寒い所では…花は枯れてしまうの…；－；\n",
    "もはよう＾－＾\n",
    "占い把握＾－＾\n",
    "[02]たん占い把握＾－＾\n",
    "占いco[07]○\n",
    "占い把握＾－＾\n",
    "占い2把握＾－＾\n",
    "[03]たんも占いね＾－＾\n",
    "占い2把握ら＾－＾真狂か真狐とかかのあ＾－＾\n",
    "人外全潜伏かしら＾－＾狂人はいなさそう？＾－＾\n",
    "対抗把握しマス＾－＾\n",
    "役職欠けて狂狐の可能性も＾－＾\n",
    ">>13狂いない可能性もある＾－＾\n",
    "対抗把握＾－＾狂>狼狐かな＾－＾\n",
    "占い2出てるから全潜伏れはないれそ＾－＾真狂めかのあ＾－＾\n",
    "[03]たんちょっと出方様子見っぽく思えたから真目下がるのあ；－；\n",
    "狂人いなくて占い欠けの狼狐らったらやばえ；－；\n",
    ">>17おんその場合は狼か狐が出てるのもあるなって＾－＾\n",
    "とりま占い先宣言してほしいお＾－＾\n",
    "今日はグレラン？＾－＾\n",
    "グレーから柱出てもらう？＾－＾吊りあんもしゆゆうのいけお＾－＾\n",
    "呪殺ないと真なのかまからん；－；\n",
    "吊りは狐先に吊らないとら＾－＾\n",
    "じゃあ[06]たん占う＾－＾\n",
    "宣言したほうがいいかんじ？＾－＾\n",
    "吊り余裕は銃殺出してもらえば増えるし対抗占いしてもらいたいかも＾－＾\n",
    "じゃあ[05]たん行きます\n",
    "漏れ吊っていいお＾－＾\n",
    "\"\"\"]\n",
    "\n",
    "truth_label = [Role.WEREWOLF, Role.VILLAGER]\n",
    "test_input= [preprocessor.preprocess_text(raw) for raw in test_input]\n",
    "\n",
    "encoding = tokenizer(\n",
    "        test_input,\n",
    "        max_length=MAX_LENGTH, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "# 推論\n",
    "with torch.no_grad():\n",
    "    output = bert_sc.forward(**encoding)\n",
    "scores = output.logits # 分類スコア\n",
    "labels_predicted = scores.argmax(-1).cpu().numpy() # スコアが最も高いラベル\n",
    "labels_predicted = [labels_list[i] for i in labels_predicted] # ラベルをラベル名に変換\n",
    "for truth, predicted in zip(truth_label, labels_predicted):\n",
    "    print(f\"truth: {truth}, predicted: {predicted}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_k2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
