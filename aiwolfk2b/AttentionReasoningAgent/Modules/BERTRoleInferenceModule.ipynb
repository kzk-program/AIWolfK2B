{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習したモデルのAttentionの可視化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from aiwolfk2b.AttentionReasoningAgent.Modules.RoleEstimationModelPreprocessor import RoleEstimationModelPreprocessor\n",
    "from aiwolfk2b.AttentionReasoningAgent.Modules.BERTRoleEstimationModel import BERTRoleEstimationModel\n",
    "from aiwolfk2b.AttentionReasoningAgent.AbstractModules import RoleEstimationResult\n",
    "from aiwolfk2b.utils.helper import load_default_GameInfo,load_default_GameSetting,load_config\n",
    "from aiwolf import Role,Agent\n",
    "\n",
    "current_dir = pathlib.Path().resolve()\n",
    "#計算に使うdeviceを取得\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"/home/meip-users/work/AI_Wolf/AIWolfK2B/aiwolfk2b/AttentionReasoningAgent/config_inference.ini\")\n",
    "game_info = load_default_GameInfo()\n",
    "game_setting = load_default_GameSetting()\n",
    "\n",
    "estimator = BERTRoleEstimationModel(config)\n",
    "estimator.initialize(game_info,game_setting)\n",
    "preprocessor:RoleEstimationModelPreprocessor = estimator.preprocessor\n",
    "labels_list = preprocessor.role_label_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可視化用関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(word, attn):\n",
    "  html_color = '#%02X%02X%02X' % (255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "  return '<span style=\"background-color: {}\">{}</span>'.format(html_color, word)\n",
    "\n",
    "# def mk_html(text,truth_label:Role,result:RoleEstimationResult):\n",
    "#   #最大確率を持つラベルを予測結果とする\n",
    "#   pred_label = max(result.probs.items(), key=lambda x: x[1])[0]\n",
    "#   html = f\"正解: {truth_label.name}<br>予測: {pred_label.name}<br>\"\n",
    "\n",
    "#   # 文章の長さ分のdarayを宣言\n",
    "#   attention_weight = result.attention_map\n",
    "\n",
    "#   seq_len = attention_weight.shape[1]\n",
    "#   all_attens = np.zeros((seq_len))\n",
    "\n",
    "#   # for i in range(12):\n",
    "#   #   all_attens += attention_weight[i, 0, :]\n",
    "#   all_attens = np.average(attention_weight[:,0,:], axis=0)\n",
    "#   #最大値を1,最小値を0として正規化\n",
    "#   min_val = all_attens.min()\n",
    "#   max_val = all_attens.max()\n",
    "#   all_attens = (all_attens - min_val) / (max_val - min_val)\n",
    "\n",
    "#   # #単語ごとにattentionの和を取る\n",
    "#   # words_all_attens = []\n",
    "#   # text_tokens:List[str] = estimator.tokenizer.tokenize(text)\n",
    "#   # word_list = []\n",
    "#   # counter = 0\n",
    "#   # for token in text_tokens:\n",
    "#   #   word_attention = 0\n",
    "#   #   ids = estimator.tokenizer.encode(token,add_special_tokens=False)\n",
    "#   #   #1 word分のattentionを足し込む\n",
    "#   #   for idx in range(counter,counter + len(ids)):\n",
    "#   #     word_attention+=all_attens[idx]\n",
    "#   #   counter += len(ids)\n",
    "#   #   words_all_attens.append(word_attention)\n",
    "    \n",
    "#   #   #一つ前と連続するか\n",
    "#   #   if token.startswith(\"##\"):\n",
    "#   #     # 単語\n",
    "#   #     part_word = token[2:]\n",
    "#   #   else:\n",
    "#   #     #連続しない場合\n",
    "#   #     part_word = token\n",
    "      \n",
    "#   #   word_list.append(part_word)\n",
    "      \n",
    "#   # for word, attn in zip(word_list,words_all_attens):\n",
    "#   #     html += highlight(word, attn)\n",
    "  \n",
    "#   text_ids = estimator.tokenizer.encode(text)\n",
    "#   for word, attn in zip(text_ids, all_attens):\n",
    "#     if estimator.tokenizer.convert_ids_to_tokens([word])[0] == \"[SEP]\":\n",
    "#       break\n",
    "#     html += highlight(estimator.tokenizer.convert_ids_to_tokens([word])[0], attn)\n",
    "  \n",
    "#   html += \"<br><br>\"\n",
    "#   return html\n",
    "\n",
    "def mk_html(text,truth_label:Role,result:RoleEstimationResult):\n",
    "  #最大確率を持つラベルを予測結果とする\n",
    "  pred_label = max(result.probs.items(), key=lambda x: x[1])[0]\n",
    "  html = f\"正解: {truth_label.name}<br>予測: {pred_label.name}<br>\"\n",
    "\n",
    "  # 文章の長さ分のdarayを宣言\n",
    "  attention_weight = result.attention_map\n",
    "\n",
    "  seq_len = attention_weight.shape[1]\n",
    "  all_attens = np.zeros((seq_len))\n",
    "\n",
    "  all_attens = np.average(attention_weight[:,0,:], axis=0)\n",
    "  #最大値を1,最小値を0として正規化\n",
    "  min_val = all_attens.min()\n",
    "  max_val = all_attens.max()\n",
    "  all_attens = (all_attens - min_val) / (max_val - min_val)\n",
    "\n",
    "  #単語ごとにattentionの和を取る\n",
    "  agg_words =[]\n",
    "  agg_attens = []\n",
    "  text_tokens:List[str] = estimator.tokenizer.tokenize(text)\n",
    "\n",
    "  \n",
    "  for idx,token in enumerate(text_tokens):\n",
    "    #print(token)\n",
    "    #一つ前と連続するか\n",
    "    if token.startswith(\"##\"):\n",
    "      # 単語\n",
    "      agg_words[-1] += token[2:]\n",
    "      agg_attens[-1] += all_attens[idx+1]\n",
    "    else:\n",
    "      #連続しない場合\n",
    "      agg_words.append(token)\n",
    "      agg_attens.append(all_attens[idx+1])\n",
    "    \n",
    "  for word, attn in zip(agg_words,agg_attens):\n",
    "      html += highlight(word, attn)\n",
    "  \n",
    "  \n",
    "  html += \"<br><br>\"\n",
    "  return html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証用テキストとその回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\"\"\"4,1,0,0,1,1,1,0\n",
    "day1\n",
    "talk:\n",
    "もはもは＾－＾\n",
    "もはもは＾－＾\n",
    "もは＾－＾占いCO[02]たん◯れした＾－＾\n",
    "もは＾－＾\n",
    "もは＾－＾寒い所では…花は枯れてしまうの…；－；\n",
    "もはよう＾－＾\n",
    "占い把握＾－＾\n",
    "[03]たん占い把握＾－＾\n",
    "占いco[08]○\n",
    "占い把握＾－＾\n",
    "占い2把握＾－＾\n",
    "[04]たんも占いね＾－＾\n",
    "占い2把握ら＾－＾真狂か真狐とかかのあ＾－＾\n",
    "人外全潜伏かしら＾－＾狂人はいなさそう？＾－＾\n",
    "対抗把握しマス＾－＾\n",
    "役職欠けて狂狐の可能性も＾－＾\n",
    ">>13狂いない可能性もある＾－＾\n",
    "対抗把握＾－＾狂>狼狐かな＾－＾\n",
    "占い2出てるから全潜伏れはないれそ＾－＾真狂めかのあ＾－＾\n",
    "[04]たんちょっと出方様子見っぽく思えたから真目下がるのあ；－；\n",
    "狂人いなくて占い欠けの狼狐らったらやばえ；－；\n",
    ">>17おんその場合は狼か狐が出てるのもあるなって＾－＾\n",
    "とりま占い先宣言してほしいお＾－＾\n",
    "今日はグレラン？＾－＾\n",
    "グレーから柱出てもらう？＾－＾吊りあんもしゆゆうのいけお＾－＾\n",
    "呪殺ないと真なのかまからん；－；\n",
    "吊りは狐先に吊らないとら＾－＾\n",
    "じゃあ[07]たん占う＾－＾\n",
    "宣言したほうがいいかんじ？＾－＾\n",
    "吊り余裕は銃殺出してもらえば増えるし対抗占いしてもらいたいかも＾－＾\n",
    "じゃあ[06]たん行きます\n",
    "漏れ吊っていいお＾－＾\n",
    "day2\n",
    "divine,1,4,HUMAN\n",
    "talk:\"\"\",\"\"\"4,1,0,0,1,1,1,0\n",
    "day1\n",
    "talk:\n",
    "もはもは＾－＾\n",
    "もはもは＾－＾\n",
    "もは＾－＾占いCO[01]たん◯れした＾－＾\n",
    "もは＾－＾\n",
    "もは＾－＾寒い所では…花は枯れてしまうの…；－；\n",
    "もはよう＾－＾\n",
    "占い把握＾－＾\n",
    "[02]たん占い把握＾－＾\n",
    "占いco[07]○\n",
    "占い把握＾－＾\n",
    "占い2把握＾－＾\n",
    "[03]たんも占いね＾－＾\n",
    "占い2把握ら＾－＾真狂か真狐とかかのあ＾－＾\n",
    "人外全潜伏かしら＾－＾狂人はいなさそう？＾－＾\n",
    "対抗把握しマス＾－＾\n",
    "役職欠けて狂狐の可能性も＾－＾\n",
    ">>13狂いない可能性もある＾－＾\n",
    "対抗把握＾－＾狂>狼狐かな＾－＾\n",
    "占い2出てるから全潜伏れはないれそ＾－＾真狂めかのあ＾－＾\n",
    "[03]たんちょっと出方様子見っぽく思えたから真目下がるのあ；－；\n",
    "狂人いなくて占い欠けの狼狐らったらやばえ；－；\n",
    ">>17おんその場合は狼か狐が出てるのもあるなって＾－＾\n",
    "とりま占い先宣言してほしいお＾－＾\n",
    "今日はグレラン？＾－＾\n",
    "グレーから柱出てもらう？＾－＾吊りあんもしゆゆうのいけお＾－＾\n",
    "呪殺ないと真なのかまからん；－；\n",
    "吊りは狐先に吊らないとら＾－＾\n",
    "じゃあ[06]たん占う＾－＾\n",
    "宣言したほうがいいかんじ？＾－＾\n",
    "吊り余裕は銃殺出してもらえば増えるし対抗占いしてもらいたいかも＾－＾\n",
    "じゃあ[05]たん行きます\n",
    "漏れ吊っていいお＾－＾\n",
    "\"\"\"]\n",
    "\n",
    "\n",
    "truth_labels = [Role.WEREWOLF, Role.VILLAGER]\n",
    "test_inputs= [preprocessor.preprocess_text(raw) for raw in test_inputs]\n",
    "\n",
    "test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "results = estimator.estimate_from_text(test_inputs)\n",
    "for i,text in enumerate(test_inputs):\n",
    "  html_output = mk_html(text,truth_labels[i],results[i])\n",
    "  display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = estimator.tokenizer\n",
    "tokenizer.tokenize(\"じゃあ[05]たん行きます\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"##あ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"じゃあ[05]たん行きます\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_string(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('じゃ',add_special_tokens=False)\n",
    "tokenizer.decode(tokenizer.encode('##あ',add_special_tokens=False),skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k2b_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
