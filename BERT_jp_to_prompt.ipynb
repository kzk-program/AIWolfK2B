{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.BERTのクラス分類で動詞を予測する分類問題のNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手始めに、BERTのクラス分類で動詞を予測することとする\n",
    "\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /storage/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (1.2.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (63.2.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /storage/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /storage/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages (from tensorboard) (1.21.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /storage/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages (from tensorboard) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (2.15.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (1.47.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from tensorboard) (2.1.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /storage/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (4.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /storage/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.9.24)\n",
      "Requirement already satisfied: zipp>=0.5 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /storage/home/robot_dev3/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /storage/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2, 16204,  1865,  4314,  7535,  4118,    81,     7,     9, 16204,\n",
      "         1865,  4314,   213,  4118,    14,  1076,    75,    13,  5705,    34,\n",
      "            3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "# データの前処理(データローダに入力できる形に整形)\n",
    "# 分類する動詞のリスト\n",
    "verb_list = ['ESTIMATE', 'COMINGOUT', 'DIVINATION', 'GUARD', 'VOTE',\n",
    "            'ATTACK', 'DIVINED', 'IDENTIFIED', 'GUARDED', 'VOTED',\n",
    "            'ATTACKED', 'AGREE', 'DISAGREE', 'Skip', 'Over' ] # REVIEW: Skip, Overをuppercaseにする必要があるかどうか\n",
    "\n",
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "# 文章をトークンに変換するトークナイザーの読み込み\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ファインチューニングで学習するためミニバッチ生成用データローダ\n",
    "# TODO: データを読み込んで前処理をする\n",
    "# 1. textをトークンに変換\n",
    "# 2. トークンにラベルを追加\n",
    "# 3. dataset_for_loaderに追加\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "\n",
    "# データを取得\n",
    "data_set_plain = pickle.load(open('dataset.pkl', 'rb'))\n",
    "for data in data_set_plain:\n",
    "    encoding = tokenizer(\n",
    "            data[1],\n",
    "            max_length=max_length, \n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "    encoding['labels'] = verb_list.index(data[0].split()[1])\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# #以下実行検証用学習データ\n",
    "# test_plain_dataset = [\n",
    "# ['ESTIMATE','私はAgent[10]が騎士だと推測する'],\n",
    "# ['COMINGOUT','Agent[01]はAgent[03]が狂人だとカミングアウトする'],\n",
    "# ['Over','Over'],\n",
    "# ['COMINGOUT','Agent[01]が占いだとカミングアウトする'],\n",
    "# ['COMINGOUT','Agent[01]はAgent[01]が占い師だとカミングアウトする'],\n",
    "# ['DIVINED','私は占った結果Agent[01]は白だった'],\n",
    "# ['DIVINED','Agent[01]が占った結果Agent[02]は人狼だった'],\n",
    "# ['GUARD','Agent[01]を護衛する'],\n",
    "# ]\n",
    "# for data in test_plain_dataset:\n",
    "#     encoding = tokenizer(\n",
    "#             data[1],\n",
    "#             max_length=max_length, \n",
    "#             padding='max_length',\n",
    "#             truncation=True\n",
    "#         )\n",
    "#     encoding['labels'] = verb_list.index(data[0])\n",
    "#     encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "#     dataset_for_loader.append(encoding)\n",
    "\n",
    "print(dataset_for_loader[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader) # ランダムにシャッフル\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n",
    "\n",
    "# データセットからデータローダを作成\n",
    "# 学習データはshuffle=Trueにする。\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=32, shuffle=True\n",
    ") \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification_pl(pl.LightningModule):\n",
    "        \n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        # model_name: Transformersのモデルの名前\n",
    "        # num_labels: ラベルの数\n",
    "        # lr: 学習率\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # 引数のnum_labelsとlrを保存。\n",
    "        # 例えば、self.hparams.lrでlrにアクセスできる。\n",
    "        # チェックポイント作成時にも自動で保存される。\n",
    "        self.save_hyperparameters() \n",
    "\n",
    "        # BERTのロード\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        \n",
    "    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数を書く。\n",
    "    # batch_idxはミニバッチの番号であるが今回は使わない。\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss) # 損失を'train_loss'の名前でログをとる。\n",
    "        return loss\n",
    "        \n",
    "    # 検証データのミニバッチが与えられた時に、\n",
    "    # 検証データを評価する指標を計算する関数を書く。\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss) # 損失を'val_loss'の名前でログをとる。\n",
    "\n",
    "    # テストデータのミニバッチが与えられた時に、\n",
    "    # テストデータを評価する指標を計算する関数を書く。\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels') # バッチからラベルを取得\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = ( labels_predicted == labels ).sum().item()\n",
    "        accuracy = num_correct/labels.size(0) #精度\n",
    "        self.log('accuracy', accuracy) # 精度を'accuracy'の名前でログをとる。\n",
    "\n",
    "    # 学習に用いるオプティマイザを返す関数を書く。\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# 学習時にモデルの重みを保存する条件を指定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='jp_to_prompt_model/',\n",
    ")\n",
    "\n",
    "# 学習の方法を指定\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=10,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory /robot-qnap-1/okubo/dl_remote/work/aiwolf/jp_to_prompt_model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name    | Type                          | Params\n",
      "----------------------------------------------------------\n",
      "0 | bert_sc | BertForSequenceClassification | 110 M \n",
      "----------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.516   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2371b05966af4f58a933c792d42b7c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (27) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6daec0dff954d2fb04092ef8f71eb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d9e20e6b7346cda3b5f920fb7153a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071ee24c7f15405687a87302c48b62f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0906161c5b71400d84fecf4de27a2a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436e67db09d740378a62e01ac9ecc16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07074547503440d385b6d3cff45646ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaa31df60a24952994294bc9377be42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3159340e5e064c04bb9021b9ac35f00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edd3558c459453a9ca9193f9d98f61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3406e8365a0e4395b788e29c921bf112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2252e0f508342cc943c740c610fb726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PyTorch Lightningモデルのロード\n",
    "model = BertForSequenceClassification_pl(\n",
    "    MODEL_NAME, num_labels=len(verb_list), lr=1e-5\n",
    ")\n",
    "\n",
    "# ファインチューニングを行う。\n",
    "trainer.fit(model, dataloader_train, dataloader_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ベストモデルのファイル:  /robot-qnap-1/okubo/dl_remote/work/aiwolf/jp_to_prompt_model/epoch=9-step=270.ckpt\n",
      "ベストモデルの検証データに対する損失:  tensor(0.0188, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 検証データで確認\n",
    "best_model_path = checkpoint.best_model_path # ベストモデルのファイル\n",
    "print('ベストモデルのファイル: ', checkpoint.best_model_path)\n",
    "print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7de70c74aa65be02\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7de70c74aa65be02\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 学習推移の確認\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1444: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `test(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at /robot-qnap-1/okubo/dl_remote/work/aiwolf/jp_to_prompt_model/epoch=9-step=270.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Loaded model weights from checkpoint at /robot-qnap-1/okubo/dl_remote/work/aiwolf/jp_to_prompt_model/epoch=9-step=270.ckpt\n",
      "/home/robot_dev3/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b6245e3d4f4a4796256c66c625807f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# テストデータで確認\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Lightningモデルのロード\n",
    "model = BertForSequenceClassification_pl.load_from_checkpoint(\n",
    "    best_model_path\n",
    ") \n",
    "\n",
    "# Transformers対応のモデルを./model_transformesに保存\n",
    "# model.bert_sc.save_pretrained('./model_transformers') \n",
    "model.bert_sc.save_pretrained('./model_verb_predicter') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存したモデルが取得できるか確認\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    './model_verb_predicter'\n",
    "    # './model_transformers'\n",
    ")\n",
    "bert_sc = bert_sc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COMINGOUT', 'IDENTIFIED', 'COMINGOUT', 'COMINGOUT', 'COMINGOUT', 'ESTIMATE', 'ESTIMATE', 'VOTE', 'GUARDED', 'DIVINED', 'ESTIMATE']\n"
     ]
    }
   ],
   "source": [
    "# 学習したモデルを検証\n",
    "text_list = [\n",
    "    \"Agent[05]はAgent[05]がボディーガードだとカミングアウトする\",\n",
    "    \"Agent[15]が襲われたAgent[10]を霊媒すると白だった\",\n",
    "    \"Agent[09]はAgent[09]が占い師だとカミングアウトする\",\n",
    "    \"Agent[05]はAgent[05]がボディーガードだと自白する\",\n",
    "    \"Agent[05]はAgent[05]がボディーガードだと暴露する\",\n",
    "    \"Agent[01]はAgent[01]が人狼だと思う\",\n",
    "    \"Agent[01]を吊るすべき\",\n",
    "    \"私はAgent[01]に投票しようと思います\",\n",
    "    \"Agent[01]がAgent[02]を護衛する\",\n",
    "    \"Agent[01]がAgent[02]を守った\", # <-汎化性能が低い。学習データに”護衛した”しかないからだと思われる\n",
    "    \"私はAgent[01]の考えに賛成です\" # <-学習データにAGREEがないので、うまく行かない\n",
    "]\n",
    "\n",
    "# データの符号化\n",
    "encoding = tokenizer(\n",
    "        text_list,\n",
    "        max_length=max_length, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "# 推論\n",
    "with torch.no_grad():\n",
    "    output = bert_sc.forward(**encoding)\n",
    "scores = output.logits # 分類スコア\n",
    "labels_predicted = scores.argmax(-1).cpu().numpy() # スコアが最も高いラベル\n",
    "labels_predicted = [verb_list[i] for i in labels_predicted] # ラベルをラベル名に変換\n",
    "print(labels_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:COMINGOUT \t Agent[05]はAgent[05]がボディーガードだとカミングアウトする\n",
      "predicted:IDENTIFIED \t Agent[15]が襲われたAgent[10]を霊媒すると白だった\n",
      "predicted:COMINGOUT \t Agent[09]はAgent[09]が占い師だとカミングアウトする\n",
      "predicted:COMINGOUT \t Agent[05]はAgent[05]がボディーガードだと自白する\n",
      "predicted:COMINGOUT \t Agent[05]はAgent[05]がボディーガードだと暴露する\n",
      "predicted:ESTIMATE \t Agent[01]はAgent[01]が人狼だと思う\n",
      "predicted:ESTIMATE \t Agent[01]を吊るすべき\n",
      "predicted:VOTE \t 私はAgent[01]に投票しようと思います\n",
      "predicted:GUARDED \t Agent[01]がAgent[02]を護衛する\n",
      "predicted:DIVINED \t Agent[01]がAgent[02]を守った\n",
      "predicted:ESTIMATE \t 私はAgent[01]の考えに賛成です\n"
     ]
    }
   ],
   "source": [
    "# 推論\n",
    "with torch.no_grad():\n",
    "    output = bert_sc.forward(**encoding)\n",
    "scores = output.logits # 分類スコア\n",
    "labels_predicted = scores.argmax(-1).cpu().numpy() # スコアが最も高いラベル\n",
    "labels_predicted = [verb_list[i] for i in labels_predicted] # ラベルをラベル名に変換\n",
    "for index, text in enumerate(text_list):\n",
    "    print(f\"predicted:{labels_predicted[index]} \\t {text}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.BERTのクラス分類で部分promptを推定するNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## やるべき手順まとめ\n",
    "### データセットの前処理\n",
    "1. コーパスデータのインポート\n",
    "2. prompt用のラベルを生成\n",
    "3. 日本語とラベルをセットにしたデータ（辞書）を学習用データとして出力\n",
    "### 学習\n",
    "1. Chapter7のモデルを使って学習\n",
    "### 検証\n",
    "1. testデータでaccuraryの計算\n",
    "2. モデルから得られた出力からpromptを復元して表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#やるべき手順まとめ\n",
    "#データセットの作成\n",
    "#   1.コーパスデータのインポート\n",
    "#   2.prompt用のラベルと日本語がセットとなったデータセットの生成\n",
    "#   3.データセットを学習用にデータローダに変換\n",
    "#学習\n",
    "#   1. Chapter7のモデルを使って学習\n",
    "#検証\n",
    "#   1.testデータでaccuraryの計算\n",
    "#   2.モデルから得られた出力からpromptを復元して表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takuya/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/takuya/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "#必要なパッケージのインポート\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# データ前処理用\n",
    "import pickle\n",
    "from aiwolfpy import ProtocolParser\n",
    "from aiwolfpy.protocol.contents import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jp2prompt用のNNモデル\n",
    "class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        # BertModelのロード\n",
    "        self.bert = BertModel.from_pretrained(model_name) \n",
    "        # 線形変換を初期化しておく\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_labels\n",
    "        ) \n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None, \n",
    "        attention_mask=None, \n",
    "        token_type_ids=None, \n",
    "        labels=None\n",
    "    ):\n",
    "        # データを入力しBERTの最終層の出力を得る。\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "        last_hidden_state = bert_output.last_hidden_state\n",
    "        \n",
    "        # [PAD]以外のトークンで隠れ状態の平均をとる\n",
    "        averaged_hidden_state = \\\n",
    "            (last_hidden_state*attention_mask.unsqueeze(-1)).sum(1) \\\n",
    "            / attention_mask.sum(1, keepdim=True)\n",
    "        \n",
    "        # 線形変換\n",
    "        scores = self.linear(averaged_hidden_state) \n",
    "        \n",
    "        # 出力の形式を整える。\n",
    "        output = {'logits': scores}\n",
    "\n",
    "        # labelsが入力に含まれていたら、損失を計算し出力する。\n",
    "        if labels is not None: \n",
    "            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n",
    "            output['loss'] = loss\n",
    "            \n",
    "        # 属性でアクセスできるようにする。\n",
    "        output = type('bert_output', (object,), output) \n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類するラベルのリスト\n",
    "subject_list = [\"Agent[01]\",\"Agent[02]\", \"Agent[03]\", \"Agent[04]\", \"Agent[05]\",\n",
    "                   \"Agent[06]\", \"Agent[07]\", \"Agent[08]\", \"Agent[09]\", \"Agent[10]\", \n",
    "                   \"Agent[11]\", \"Agent[12]\", \"Agent[13]\", \"Agent[14]\", \"Agent[15]\",\"UNSPEC\",\"ANY\"] #TODO:ここ周りテキトーにやってる。ホントは分類ではなく値自身を使えばいいはず\n",
    "verb_list = ['ESTIMATE', 'COMINGOUT', 'DIVINATION', 'GUARD', 'VOTE',\n",
    "            'ATTACK', 'DIVINED', 'IDENTIFIED', 'GUARDED', 'VOTED',\n",
    "            'ATTACKED', 'AGREE', 'DISAGREE', 'Skip', 'Over' ] # REVIEW: Skip, Overをuppercaseにする必要があるかどうか\n",
    "target_list = subject_list\n",
    "species_list = ['HUMAN',\"WEREWOLF\",\"ANY\"]\n",
    "role_list = ['VILLAGER','SEER', 'MEDIUM','BODYGUARD','WEREWOLF','POSSESSED','ANY']\n",
    "talk_number_list = [str(i) for i in range(1, 16)] #TODO:ここ周りテキトーにやってる。ホントは分類ではなく値自身を使えばいいはず\n",
    "\n",
    "\n",
    "# ラベルの数\n",
    "label_size = len(subject_list) + len(verb_list) + len(target_list) \\\n",
    "                    + len(role_list) + len(species_list) + len(talk_number_list)\n",
    "# ラベル用ディクショナリの生成\n",
    "cum_sum = 0\n",
    "subject_start_index = cum_sum\n",
    "subject_dict = {subject_list[i]:i for i in range(len(subject_list))}\n",
    "cum_sum += len(subject_list)\n",
    "subject_end_index = cum_sum\n",
    "verb_start_index = cum_sum\n",
    "\n",
    "verb_dict = {verb_list[i]:i+cum_sum for i in range(len(verb_list))}\n",
    "cum_sum += len(verb_list)\n",
    "verb_end_index = cum_sum\n",
    "target_start_index = cum_sum\n",
    "\n",
    "target_dict = {target_list[i]:i+cum_sum for i in range(len(target_list))}\n",
    "cum_sum += len(target_list)\n",
    "target_end_index = cum_sum\n",
    "role_start_index = cum_sum\n",
    "\n",
    "role_dict = {role_list[i]:i+cum_sum for i in range(len(role_list))}\n",
    "cum_sum += len(role_list)\n",
    "role_end_index = cum_sum\n",
    "species_start_index = cum_sum\n",
    "\n",
    "species_dict = {species_list[i]:i+cum_sum for i in range(len(species_list))}\n",
    "cum_sum += len(species_list)\n",
    "species_end_index = cum_sum\n",
    "talk_number_start_index = cum_sum\n",
    "\n",
    "talk_number_dict = {talk_number_list[i]:i+cum_sum for i in range(len(talk_number_list))}\n",
    "cum_sum += len(talk_number_list)\n",
    "talk_end_index = cum_sum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVTRSNの順番に並べて出力のラベルとする\n",
    "def calc_label(prompt):\n",
    "    content = ProtocolParser.parse(prompt)\n",
    "\n",
    "    labels = [0 for i in range(label_size)]\n",
    "    \n",
    "    #文型を分類してラベルを追加\n",
    "    if type(content) == SVTRContent:\n",
    "        labels[subject_dict[content.subject]] = 1\n",
    "        labels[verb_dict[content.verb]] = 1\n",
    "        labels[target_dict[content.target]] = 1\n",
    "        labels[role_dict[content.role]] = 1\n",
    "    elif type(content) == SVTContent:\n",
    "        labels[subject_dict[content.subject]] = 1\n",
    "        labels[verb_dict[content.verb]] = 1\n",
    "        labels[target_dict[content.target]] = 1\n",
    "    elif type(content) == SVTSContent:\n",
    "        labels[subject_dict[content.subject]] = 1\n",
    "        labels[verb_dict[content.verb]] = 1\n",
    "        labels[target_dict[content.target]] = 1\n",
    "        labels[species_dict[content.species]] = 1\n",
    "    elif type(content) == ControlContent:\n",
    "        labels[subject_dict[content.subject]] = 1\n",
    "        labels[verb_dict[content.verb]] = 1\n",
    "    else:\n",
    "        #今の検証ではSyntaxエラーを投げる\n",
    "        #TODO: 別構文も判定できるようにするためにはこの条件分岐を変える（追加する）必要あり\n",
    "        print(prompt)\n",
    "        raise SyntaxError(\"Syntax Error\")\n",
    "    \n",
    "    return labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2, 16204,  1865,  4314,  7535,  4118,    81,     7,     9, 16204,\n",
      "         1865,  4314,   213,  4118,    14,  1076,    75,    13,  5705,    34,\n",
      "            3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# データの前処理(データローダに入力できる形に整形)\n",
    "\n",
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "# 文章をトークンに変換するトークナイザーの読み込み\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ファインチューニングで学習するためミニバッチ生成用データローダの作成\n",
    "\n",
    "# データを取得\n",
    "dataset_plain = pickle.load(open('dataset.pkl', 'rb'))\n",
    "dataset = []\n",
    "# データを前処理\n",
    "for data in dataset_plain:\n",
    "    prompt = data[0]\n",
    "    jp_script= data[1]\n",
    "\n",
    "    #ラベルを生成\n",
    "    labels = calc_label(prompt)\n",
    "    sample = {'text':jp_script, 'labels':labels}\n",
    "    #データセットに追加\n",
    "    dataset.append(sample)\n",
    "    \n",
    "# データをトークンに変換\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "\n",
    "for data in dataset:\n",
    "    text = data['text']\n",
    "    labels = data['labels']\n",
    "    encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length, \n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "    encoding['labels'] = labels\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# #以下実行検証用学習データ\n",
    "\n",
    "# #以下実行検証用学習データ\n",
    "# test_plain_dataset = [\n",
    "# ['ESTIMATE','私はAgent[10]が騎士だと推測する'],\n",
    "# ['COMINGOUT','Agent[01]はAgent[03]が狂人だとカミングアウトする'],\n",
    "# ['Over','Over'],\n",
    "# ['COMINGOUT','Agent[01]が占いだとカミングアウトする'],\n",
    "# ['COMINGOUT','Agent[01]はAgent[01]が占い師だとカミングアウトする'],\n",
    "# ['DIVINED','私は占った結果Agent[01]は白だった'],\n",
    "# ['DIVINED','Agent[01]が占った結果Agent[02]は人狼だった'],\n",
    "# ['GUARD','Agent[01]を護衛する'],\n",
    "# ]\n",
    "# for data in test_plain_dataset:\n",
    "#     encoding = tokenizer(\n",
    "#             data[1],\n",
    "#             max_length=max_length, \n",
    "#             padding='max_length',\n",
    "#             truncation=True\n",
    "#         )\n",
    "#     encoding['labels'] = verb_list.index(data[0])\n",
    "#     encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "#     dataset_for_loader.append(encoding)\n",
    "\n",
    "print(dataset_for_loader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader) \n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n",
    "\n",
    "#　データセットからデータローダを作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=32, shuffle=True\n",
    ") \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/takuya/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory /home/takuya/HDD1/work/AIWolfK2B/model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                    | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | bert_scml | BertForSequenceClassificationMultiLabel | 110 M \n",
      "----------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.697   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feb4b2c903d439388a6c6ab90d3711d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takuya/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/takuya/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/takuya/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (27) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a026f24863544273b0d462f8c9f17183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea11075c6c5a4a36a623250f3be2ddc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e751018cc074f86b4ac3ab8cc8d297a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff07ff8e44b4ef8a66f96b969c7431c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d7b824e8cd45a0a485c90c72d37f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c5d244dff14d06a9a46000e1345f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eaba2ed743f4d8aaee789df3ae3a6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9f6c9dfa31434f905484e45bc3411f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39ca247d47245f783999e770a0f643a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffe8ebe268642b9a07f968d54112e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951b661cecbb433ebdf501f824b5b7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f405144a1c8244aaafcd534a35a728ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298abcc4d1184234a834d9fcac52f659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a851e9c4b048a68414829a9db66a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e9d981c0d845fcb33418edd4de5307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acacc3a3982401aaec642e516b9ee02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d82088005a424eb33c43675acabf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52eaadbf3ec4e17b42b361f9692fd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46a4628086f4ac484f31d920006f4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7c83583ee14959aa2e1705d20168e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77ce969584249c4bf623da9bb9c2d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef2cf969e9940beaa0dfeeb82a96e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc743047229499dae2b178636d1247e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d23627fb4947c3b49cfad98737323b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06a815457c547ccb7d94385da8e5723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5711c901f634094bf4ee92380a8ca0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4861d762807a42caa3a9bc99b18074d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ad5fac504847b8b99baa072d110bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89ac7f557df42c198084e5d34d899ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df0493179564341ac616850b1b174bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c89300772f4f88af5329689f485501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf425b66cee4b58b9efa7396ea0f37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69fb4908a4b4c61a2729e5ec21aa368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a9301588344f3da3a91a05484cda51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bf6f4782874dfab040cf5c0b369800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3570df6703a54622a0786fdbd3fd2191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac09bcc3eb8e4ef7a7943ecab0a8e06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3109441740984530a882c28226813ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d196b3e2094083ac7b8f02cd13b3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bd17b5b0ef43559710bc43e1ebd1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336dd93e1ac5475a89c678d0e892a531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c45ad1ba8254ae689fb5b69e2669749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309fbfeb272c480e9c2a6a5a412f9dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381f059877214dfeaf71d43c57e4d347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14716dde74c418099ee41944d5626e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3e2fe944af4b659ef81f3a58163438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d371879ceca240aab322a7c97068adb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0d33b7e9694e6b91c7c1e03dfe0529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de946218518844ddbdc624cb58f783f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4936b0c239403fb8250a0785fa9b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f466be85510498b9e25fb6696208df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6161cce3d25c41118609d09675e97fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01aaa3af28a54144b52ff5b1ab504f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a969ff326665468da8f7e91cc196a7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f541346d55cf44618f7380b9cb8cb7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ca648a2e5c4eb0a3fd2c8599150af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7e1ef54eb04ab8bc8eecfbbbf58f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fb8ac634fa423bbd1aeca17e1a6a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0220a77b2aa849bd89fef1a2948e7774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0059a84f90740d3b63e96ad840d9fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be130b5848a4c3ca2b9f94f912cd102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649911d06d4e4a4ea5a4e0ca50763428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45597bd1a7a8465eb3854d7e37e7aae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a04975d3cd4a129ffff91d1062008d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20918ef0cb0a476e917f14c9678aaf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e834d4abf764159b2619841c1cd550c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90d9c5aea744d2580329b7494f108a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3f6192ab404e0d9bea7b84970175c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f58c45e4a54fee8e28e5891e0e525b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a88dfe4d8e45ce91a6954ef1ae8f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353fc4ac30364699a8f5db434d22b856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e404e5bc79b4d1791a633e87d96f5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ccbc84a7c94e518a11b09510f8f4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f72340d76e471cba8f4db703e02bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3d55b679314169930fb8f3744641b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0623291704e8440f81ac347c26a17079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15b3b7ae8f044e08657093580cbba65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c296be6db9d54544bf9e592dca9c41be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3b08d92cb14202ae5e7fdb5efe7f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a96f09a27545c19a50a2c4c761ca8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb252054a26c4c64ac7d9d8e9d0bb2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9229e18e1649d89caa4d31736ab473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ade3f4bb0f04f709575cdbb25d5c52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e822019a2b1e4fa2ad8c4b318620df8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093907e888d34bff9240f1921fa37f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d21be1be144915ba90f64caf1b66cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ec297d669f4dc9b4577b89230fba32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47755932ed414bd7ba6642ddcdf9cabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef1d288d63e497d945c7facb96df3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7258a993e11945258976311d9204de78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10be5c2fd9e946089f2e04be33e0ba0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1988c4630e4d13a46c13c856cac436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c3999efe1544369f74a60c3bc6fdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc1cbebbee84c0088c24ab3d127064b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09890120579449dfbcd41175d7ea3053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95eece295b6d47e4bc2183e408f2c259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f35cafb0e7344a383fdf9879f69ce34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08c5228beba45399a8c4ce3790321ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c9698a7f2e4069abb2c2b018dda894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff7c1e471b046f4a825bba9309c5f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7d4d1432f240d0b8f010d9812ac4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine-tuning用のクラスを定義\n",
    "class BertForSequenceClassificationMultiLabel_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        self.bert_scml = BertForSequenceClassificationMultiLabel(\n",
    "            model_name, num_labels=num_labels\n",
    "        ) \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels')\n",
    "        output = self.bert_scml(**batch)\n",
    "        scores = output.logits\n",
    "        labels_predicted = ( scores > 0 ).int()\n",
    "        num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n",
    "        accuracy = num_correct/scores.size(0)\n",
    "        self.log('accuracy', accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "# 学習\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=100,\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassificationMultiLabel_pl(\n",
    "    MODEL_NAME, \n",
    "    num_labels=label_size, \n",
    "    lr=1e-5\n",
    ")\n",
    "trainer.fit(model, dataloader_train, dataloader_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takuya/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1444: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `test(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at /home/takuya/HDD1/work/AIWolfK2B/model/epoch=99-step=2700.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/takuya/HDD1/work/AIWolfK2B/model/epoch=99-step=2700.ckpt\n",
      "/home/takuya/anaconda3/envs/aiwolf/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d115255955f484eb9ff4f5185680f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        accuracy            0.9928571581840515\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# 検証1: テストデータで評価\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得られたスコアからプロンプトを生成\n",
    "def calc_prompt_from_scores(scores):\n",
    "    #scoresのラベルはSVTRSNの順番に並べられている\n",
    "    prompt = []\n",
    "    subject_index = scores[subject_start_index:subject_end_index].argmax().item()\n",
    "    prompt.append(subject_list[subject_index])\n",
    "    \n",
    "    \n",
    "    verb_index = scores[verb_start_index:verb_end_index].argmax().item()\n",
    "    prompt.append(verb_list[verb_index])\n",
    "    #動詞で分類\n",
    "    SVTR_verbs = ['ESTIMATE', 'COMINGOUT']\n",
    "    SVTS_verbs = ['DIVINED', 'IDENTIFIED']\n",
    "    SVT_verbs = ['DIVINATION', 'GUARD', 'VOTE','ATTACK', 'GUARDED', 'VOTED','ATTACKED']\n",
    "    SV_verbs = ['Skip', 'Over']  \n",
    "    verb = verb_list[verb_index]\n",
    "    \n",
    "    if verb in SVTR_verbs:\n",
    "        target_index = scores[target_start_index:target_end_index].argmax().item()\n",
    "        prompt.append(target_list[target_index])\n",
    "        role_index = scores[role_start_index:role_end_index].argmax().item()\n",
    "        prompt.append(role_list[role_index])\n",
    "    elif verb in SVTS_verbs:\n",
    "        target_index = scores[target_start_index:target_end_index].argmax().item()\n",
    "        prompt.append(target_list[target_index])\n",
    "        species_index = scores[species_start_index:species_end_index].argmax().item()\n",
    "        prompt.append(species_list[species_index])\n",
    "    elif verb in SVT_verbs:\n",
    "        target_index = scores[target_start_index:target_end_index].argmax().item()\n",
    "        prompt.append(target_list[target_index])\n",
    "    elif verb in SV_verbs:\n",
    "        pass\n",
    "    else:\n",
    "        #今の検証ではSyntaxエラーを投げる\n",
    "        #TODO: 別構文も判定できるようにするためにはこの条件分岐を変える（追加する）必要あり\n",
    "        print(verb)\n",
    "        raise SyntaxError(\"Syntax Error\")\n",
    "    \n",
    "    prompt = ' '.join(prompt)\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "入力：Agent[03]はAgent[08]が狼だと推測する\n",
      "予測：Agent[03] ESTIMATE Agent[08] WEREWOLF\n",
      "--\n",
      "入力：Agent[06]はAgent[06]が占い師だとカミングアウトする\n",
      "予測：Agent[06] COMINGOUT Agent[06] SEER\n",
      "--\n",
      "入力：Agent[12]が占った結果Agent[10]は人狼だった\n",
      "予測：Agent[12] DIVINED Agent[10] WEREWOLF\n",
      "--\n",
      "入力：Agent[12]が占った結果Agent[10]は人間だった\n",
      "予測：Agent[12] DIVINED Agent[10] HUMAN\n",
      "--\n",
      "入力：Agent[08]が襲われたAgent[05]を霊媒すると人間だった\n",
      "予測：Agent[08] IDENTIFIED Agent[05] HUMAN\n",
      "--\n",
      "入力：Agent[05]はAgent[10]を護衛した\n",
      "予測：Agent[05] GUARDED Agent[10]\n",
      "--\n",
      "入力：Agent[10]はAgent[12]に投票する\n",
      "予測：Agent[10] VOTE Agent[12]\n",
      "--\n",
      "入力：Agent[06]はAgent[08]が狼だと思う\n",
      "予測：Agent[06] ESTIMATE Agent[08] WEREWOLF\n",
      "--\n",
      "入力：私が占い師です\n",
      "予測：Agent[10] COMINGOUT Agent[05] SEER\n",
      "--\n",
      "入力：Agent[12]が占った結果、Agent[10]は人狼でした\n",
      "予測：Agent[12] DIVINED Agent[10] WEREWOLF\n",
      "--\n",
      "入力：Agent[12]が占った結果、Agent[10]は人間でした\n",
      "予測：Agent[12] DIVINED Agent[10] HUMAN\n",
      "--\n",
      "入力：Agent[12]がAgent[05]を霊媒すると人間でした\n",
      "予測：Agent[12] IDENTIFIED Agent[05] HUMAN\n",
      "--\n",
      "入力：Agent[12]はAgent[10]を守った\n",
      "予測：Agent[12] VOTE Agent[10]\n",
      "--\n",
      "入力：Agent[10]はAgent[12]に投票します\n",
      "予測：Agent[10] VOTE Agent[12]\n",
      "--\n",
      "入力：Agent[08]が狼だと思う\n",
      "予測：Agent[08] ESTIMATE Agent[08] WEREWOLF\n",
      "--\n",
      "入力：私が占い師です\n",
      "予測：Agent[10] COMINGOUT Agent[05] SEER\n",
      "--\n",
      "入力：占った結果、Agent[10]は人狼でした\n",
      "予測：Agent[10] DIVINED Agent[10] WEREWOLF\n",
      "--\n",
      "入力：占った結果、Agent[10]は人間でした\n",
      "予測：Agent[10] DIVINED Agent[10] HUMAN\n",
      "--\n",
      "入力：Agent[05]を霊媒すると人間でした\n",
      "予測：Agent[05] IDENTIFIED Agent[04] HUMAN\n",
      "--\n",
      "入力：私はAgent[10]を守った\n",
      "予測：Agent[10] DIVINED Agent[10] WEREWOLF\n",
      "--\n",
      "入力：私はAgent[12]に投票します\n",
      "予測：Agent[12] VOTE Agent[12]\n"
     ]
    }
   ],
   "source": [
    "# 検証2: 適当な入力分を用意してprompt予測をし結果を確認\n",
    "# 入力する文章\n",
    "text_list = [\n",
    "    \"Agent[03]はAgent[08]が狼だと推測する\",\n",
    "    \"Agent[06]はAgent[06]が占い師だとカミングアウトする\",\n",
    "    \"Agent[12]が占った結果Agent[10]は人狼だった\",\n",
    "    \"Agent[12]が占った結果Agent[10]は人間だった\",\n",
    "    'Agent[08]が襲われたAgent[05]を霊媒すると人間だった',\n",
    "    'Agent[05]はAgent[10]を護衛した',\n",
    "    \"Agent[10]はAgent[12]に投票する\",\n",
    "    \n",
    "    \"Agent[06]はAgent[08]が狼だと思う\",\n",
    "    \"私が占い師です\",\n",
    "    \"Agent[12]が占った結果、Agent[10]は人狼でした\",\n",
    "    \"Agent[12]が占った結果、Agent[10]は人間でした\",\n",
    "    'Agent[12]がAgent[05]を霊媒すると人間でした',\n",
    "    'Agent[12]はAgent[10]を守った',\n",
    "    \"Agent[10]はAgent[12]に投票します\",\n",
    "    \n",
    "    \"Agent[08]が狼だと思う\",\n",
    "    \"私が占い師です\",\n",
    "    \"占った結果、Agent[10]は人狼でした\",\n",
    "    \"占った結果、Agent[10]は人間でした\",\n",
    "    'Agent[05]を霊媒すると人間でした',\n",
    "    '私はAgent[10]を守った',\n",
    "    \"私はAgent[12]に投票します\",\n",
    "    \n",
    "]\n",
    "\n",
    "# モデルのロード\n",
    "best_model_path = checkpoint.best_model_path\n",
    "model = BertForSequenceClassificationMultiLabel_pl.load_from_checkpoint(best_model_path)\n",
    "bert_scml = model.bert_scml.cuda()\n",
    "\n",
    "# データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list, \n",
    "    padding = 'longest',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "# BERTへデータを入力し分類スコアを得る。\n",
    "with torch.no_grad():\n",
    "    output = bert_scml(**encoding)\n",
    "scores = output.logits\n",
    "labels_predicted = ( scores > 0 ).int().cpu().numpy().tolist()\n",
    "\n",
    "# 結果を表示\n",
    "for text, score in zip(text_list, scores.double().cpu().numpy()):\n",
    "    print('--')\n",
    "    print(f'入力：{text}')\n",
    "    print(f'予測：{calc_prompt_from_scores(score)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers対応のモデルを./jp2prompt_modelに保存\n",
    "out_model_dir = './jp2prompt_model'\n",
    "\n",
    "import datetime\n",
    "today = datetime.date.today()\n",
    "out_filename = format(today, '%Y%m%d')\n",
    "\n",
    "torch.save(model.state_dict(),out_model_dir + '/' + f'{out_filename}.pth')\n",
    "#model.bert_scml.save_pretrained('./model_transformers') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルのロード\n",
    "best_model_path = \"./jp2prompt_model/20221225.pth\"\n",
    "load_model = BertForSequenceClassificationMultiLabel_pl(MODEL_NAME, num_labels=label_size, lr=1e-5)\n",
    "load_model.load_state_dict(torch.load(best_model_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiwolf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "732d44a20ec9c660a2ce6561bf7e4a8c026e1d487809526863d119e0d4d1f931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
